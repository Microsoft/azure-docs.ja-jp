---
title: Azure Data Factory の価格を実例から理解する
description: このアーティクルでは、詳細例を用いて Azure Data Factory の価格モデルの説明と実演を行います
author: dcstwh
ms.author: weetok
ms.reviewer: jburchel
ms.service: data-factory
ms.topic: conceptual
ms.date: 09/14/2020
ms.openlocfilehash: 3bb9574c74aaa3c2589d0ca93fb906168ca99095
ms.sourcegitcommit: f611b3f57027a21f7b229edf8a5b4f4c75f76331
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/22/2021
ms.locfileid: "104783372"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>Data Factory の価格を実例から理解する

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

このアーティクルでは、詳細例を用いて Azure Data Factory の価格モデルの説明と実演を行います。

> [!NOTE]
> 次の例で使用されている価格は仮定的なものであり、実際の価格を暗示することを意図するものではありません。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>データを AWS S3 から Azure Blob Storage に1時間に1度コピーする

このシナリオでは、データを AWS S3 から Azure Blob　Storage に1時間に1度コピーします。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3からコピーするデータの入力データセットのコピーアクティビティ。

2. Azure Storage 上のデータの出力データセット。

3. パイプラインを毎時実行するスケジュールトリガー。

   ![図には、スケジュール トリガーを伴うパイプラインが示されています。 パイプラインでは、コピー アクティビティが入力データセットに送信され、ここから AWS S3 のリンクされたサービスに送信されます。また、コピー アクティビティは出力データセットにも送信され、ここから Azure Storate のリンクされたサービスに送信されます。](media/pricing-concepts/scenario1.png)

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 2つの読み取り/書き込みエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 2 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 1 つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 2 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 1 つ) |

**シナリオ価格の合計: $0.16811**

- Data Factory の操作 = **$0.0001**
  - 読み取り／書き込み = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 2\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.168**
  - アクティビティの実行 = 001\*2 = 0.002 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>データをコピーし、 Azure Databricks で 1 時間ごとに変換する

このシナリオでは、1 時間ごとのスケジュールでデータを AWS S3 から Azure Blob ストレージにコピーし、 Azure Databricks でデータを変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3 からコピーされるデータの入力データセットと、Azure　Storage のデータの出力データセットを含む 1 つのコピーアクティビティです。
2. データ変換のための１つの Azure Databricks アクティビティ。
3. パイプラインを 1 時間ごとに実行する 1 つのスケジュールトリガー。

![図には、スケジュール トリガーを伴うパイプラインが示されています。 パイプラインでは、コピー アクティビティが入力データセット、出力データセット、および Azure Databricks で実行される DataBricks アクティビティに送信されます。 入力データセットは、AWS S3 のリンクされたサービスに送信されます。 出力データセットは、Azure Storage のリンクされたサービスに送信されます。](media/pricing-concepts/scenario2.png)

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 3 つの書き込み/読み取りエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 3つのアクティビティの実行(トリガーの実行用に1つ、アクティビティの実行用に2つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 3 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 2 つ) |
| Databricksアクティビティ Assumption を実行する: 実行時間 = 10 分 | 10分間の外部パイプラインアクティビティの実行 |

**シナリオ価格の合計: $0.16916**

- Data Factory の操作 = **$0.00012**
  - 読み取り／書き込み = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 3\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.16904**
  - アクティビティの実行 = 001\*3 = 0.003 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)
  - 外部パイプラインアクティビティ = $0.000041 （日割の実行時間の10分間です。 $0.00025/時間、 Azure 統合ランタイム)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>データをコピーし動的パラメーターで1時間ごとに変換する

このシナリオでは、1時間ごとのスケジュールでデータを AWS S3 から Azure Blob Storage にコピーし、 Azure Databricks （スクリプト内に動的パラメーターのある）でデータを変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3からコピーされるデータの入力データセット、Azure Storageのデータの出力データセットを含む1つのコピーアクティビティです。
2. 変換スクリプトにパラメーターを動的に渡すための1つの検索アクティビティ。
3. データ変換のための１つの Azure Databricks アクティビティ。
4. パイプラインを 1 時間ごとに実行する 1 つのスケジュールトリガー。

![図には、スケジュール トリガーを伴うパイプラインが示されています。 パイプラインでは、コピー アクティビティが入力データセット、出力データセット、およびルックアップ アクティビティに送信されます。ルックアップ アクティビティは DataBricks アクティビティに送信され、このアクティビティは Azure Databricks で実行されます。 入力データセットは、AWS S3 のリンクされたサービスに送信されます。 出力データセットは、Azure Storage のリンクされたサービスに送信されます。](media/pricing-concepts/scenario3.png)

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 3 つの書き込み/読み取りエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 4 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 3 つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 4 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 3 つ) |
| 検索アクティビティ Assumption を実行する: 実行時間 = 1 分 | 1 分間のパイプラインアクティビティの実行 |
| Databricksアクティビティ Assumption を実行する: 実行時間 = 10 分 | 10 分間の外部パイプラインアクティビティの実行 |

**シナリオ価格の合計: $0.17020**

- Data Factory の操作 = **$0.00013**
  - 読み取り／書き込み = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 4\*000005 = $0.00002 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.17007**
  - アクティビティの実行 = 001\*4 = 0.004 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)
  - パイプラインアクティビティ = $0.00003 （日割りの実行時間の1分間です。 $0.002/時間、 Azure 統合ランタイム)
  - 外部パイプラインアクティビティ = $0.000041 （日割の実行時間の10分間です。 $0.00025/時間、 Azure 統合ランタイム)

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>通常の平日にマッピング データ フロー デバッグを使用する

Sam は、データ エンジニアとして、毎日マッピング データ フローを設計、構築、テストする責任があります。 Sam は、朝に ADF UI にログインし、データ フローのデバッグ モードを有効にします。 デバッグ セッションの既定の TTL は、60 分です。 Sam は 1 日 8 時間働いているので、デバッグ セッションは期限切れになりません。 したがって、Sam のその日の料金は次のようになります。

**8 (時間) x 8 (コンピューティング最適化コア数) x $0.193 = $12.35**

同時に、別のデータ エンジニアの Chris も、データ プロファイルおよび ETL 設計作業のために ADF ブラウザー UI にログインします。 Chris は、Sam のように終日 ADF で作業するわけではありません。 Chris は、上記の Sam と同じ日の同じ期間に 1 時間、データ フロー デバッガーを使用する必要があるだけです。 デバッグの使用に対して Chris が負う料金は次のとおりです。

**1 (時間) x 8 (汎用コア数) x $0.274 = $2.19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>マッピング データ フローを使用して BLOB ストア内のデータを変換する

このシナリオでは、時間単位のスケジュールで BLOB ストア内のデータを ADF マッピング データ フローで視覚的に変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. 変換ロジックを含む Data Flow アクティビティ。

2. Azure Storage 上のデータの入力データセット。

3. Azure Storage 上のデータの出力データセット。

4. パイプラインを毎時実行するスケジュールトリガー。

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 2つの読み取り/書き込みエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 2 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 1 つ) |
| Data Flow の前提条件: 実行時間 = 10 分 + TTL 10 分 | 10 \* 16 コアの一般コンピューティング (TTL 10) |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 2 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 1 つ) |

**シナリオ価格の合計: $1.4631**

- Data Factory の操作 = **$0.0001**
  - 読み取り／書き込み = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 2\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインのオーケストレーション &amp; 実行 = **$1.463**
  - アクティビティの実行 = 001\*2 = 0.002 [1 実行 = $1/1000 = 0.001]
  - データ フロー アクティビティ = $1.461: 20 分間の時間割り (実行時間 10 分 + TTL 10 分)。 Azure Integration Runtime で $0.274/時間、16 コアの一般コンピューティング

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure Data Factory マネージド VNET でのデータ統合
このシナリオでは、Azure Blob Storage にある元のファイルを削除し、Azure SQL Database から Azure Blob Storage にデータをコピーします。 この実行を、異なるパイプラインで 2 回行います。 これら 2 つのパイプラインでの実行時間は重なっています。
![Scenario4](media/pricing-concepts/scenario-4.png) シナリオを実現するには、次の項目を含む 2 つのパイプラインを作成する必要があります。
  - パイプライン アクティビティ – 削除アクティビティ。
  - Azure Blob Storage からコピーするデータに対する入力データセットのコピー アクティビティ。
  - Azure SQL Database 上のデータに対する出力データセット。
  - パイプラインを実行するためのスケジュール トリガー。


| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 4 つの書き込み/読み取りエンティティ |
| データセットを作成する | 8 つの読み取り/書き込みエンティティ(データセットの作成用に 4 つ、リンクされたサービスの参照用に 4 つ) |
| パイプラインを作成する | 6 つの読み取り/書き込みエンティティ (パイプラインの作成用に 2 つ、データセットの参照用に 4 つ) |
| パイプラインを取得する | 2つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 6 つのアクティビティ実行 (トリガーの実行用に 2 つ、アクティビティの実行用に 4 つ) |
| 削除アクティビティの実行: 各実行時間 = 5 分。最初のパイプラインでの削除アクティビティの実行は、10:00 AM UTC から 10:05 AM UTC までです。 2 番目のパイプラインでの削除アクティビティの実行は、10:02 AM UTC から 10:07 AM UTC までです。|マネージド VNET での合計 7 分のパイプライン アクティビティ実行。 パイプライン アクティビティでは、マネージド VNET で最大 50 個の同時実行がサポートされています。 |
| Data Assumption をコピーする: 各実行時間 = 10 分。最初のパイプラインでのコピー実行は、10:06 AM UTC から 10:15 AM UTC までです。 2 番目のパイプラインでの削除アクティビティの実行は、10:08 AM UTC から 10:17 AM UTC までです。 | 10 * 4 つの Azure Integration Runtime (既定の DIU 設定 = 4) データ統合ユニットとコピー パフォーマンスの最適化についての詳細は、[こちらの記事](copy-activity-performance.md)を参照してください |
| パイプライン監視の仮定:実行が 2 回だけ発生 | 6 つの取得された監視実行レコード (パイプラインの実行用に 2 つ、アクティビティの実行用に 4 つ) |


**シナリオ価格の合計: $0.45523**

- Data Factory の操作 = $0.00023
  - 読み取り/書き込み = 20*00001 = $0.0002 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 6*000005 = $0.00003 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインのオーケストレーションと実行 = $0.455
  - アクティビティの実行 = 0.001*6 = 0.006 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.333 (実行時間 10 分に対する日割り。 $0.25/時間、 Azure 統合ランタイム)
  - パイプライン アクティビティ = $0.116 （実行時間 7 分に対する日割り。 $1/時間、Azure Integration Runtime)

> [!NOTE]
> これらの価格はあくまでも例です。

**FAQ**

Q:50 より多くのパイプライン アクティビティを実行する場合、これらのアクティビティを同時に実行することはできますか?

A:許可される同時実行パイプライン アクティビティは最大 50 個です。  51 番目のパイプライン アクティビティは、"空きスロット" が開放されるまでキューに登録されます。 外部アクティビティについても同じです。 許可される同時実行外部アクティビティは最大 800 個です。

## <a name="next-steps"></a>次のステップ

Azure Data Factory の価格を理解したところで、始めることができます!

- [Azure Data Factory UIを使用してData Factoryを作成する](quickstart-create-data-factory-portal.md)

- [Azure Data Factory の概要](introduction.md)

- [Azure Data Factory でのビジュアルの作成](author-visually.md)
