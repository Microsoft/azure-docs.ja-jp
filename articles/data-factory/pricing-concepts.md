---
title: Azure Data Factory の価格を実例から理解する
description: このアーティクルでは、詳細例を用いて Azure Data Factory の価格モデルの説明と実演を行います
documentationcenter: ''
author: djpmsft
ms.author: daperlov
manager: jroth
ms.reviewer: maghan
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 12/27/2019
ms.openlocfilehash: 9d96e3f7d127f4839592e766537cbdb07cc697dc
ms.sourcegitcommit: 877491bd46921c11dd478bd25fc718ceee2dcc08
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/02/2020
ms.locfileid: "81414938"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>Data Factory の価格を実例から理解する

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

このアーティクルでは、詳細例を用いて Azure Data Factory の価格モデルの説明と実演を行います。

> [!NOTE]
> 次の例で使用されている価格は仮定的なものであり、実際の価格を暗示することを意図するものではありません。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>データを AWS S3 から Azure Blob Storage に1時間に1度コピーする

このシナリオでは、データを AWS S3 から Azure Blob　Storage に1時間に1度コピーします。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3からコピーするデータの入力データセットのコピーアクティビティ。

2. Azure Storage 上のデータの出力データセット。

3. パイプラインを毎時実行するスケジュールトリガー。

   ![Scenario1](media/pricing-concepts/scenario1.png)

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 2つの読み取り/書き込みエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 2 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 1 つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 2つの監視実行レコードの再試行 (パイプラインの実行用に 1 つ、アクティビティの実行用に 1 つ) |

**シナリオ価格の合計: $0.16811**

- Data Factory の操作 = **$0.0001**
  - 読み取り／書き込み = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 2\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.168**
  - アクティビティの実行 = 001\*2 = 0.002 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>データをコピーし、 Azure Databricks で 1 時間ごとに変換する

このシナリオでは、1 時間ごとのスケジュールでデータを AWS S3 から Azure Blob ストレージにコピーし、 Azure Databricks でデータを変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3 からコピーされるデータの入力データセットと、Azure　Storage のデータの出力データセットを含む 1 つのコピーアクティビティです。
2. データ変換のための１つの Azure Databricks アクティビティ。
3. パイプラインを 1 時間ごとに実行する 1 つのスケジュールトリガー。

![Scenario2](media/pricing-concepts/scenario2.png)

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 3 つの書き込み/読み取りエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 3つのアクティビティの実行(トリガーの実行用に1つ、アクティビティの実行用に2つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 3つの監視実行レコードの再試行(パイプラインの実行用に1つ、アクティビティの実行用に2つ) |
| Databricksアクティビティ Assumption を実行する: 実行時間 = 10 分 | 10分間の外部パイプラインアクティビティの実行 |

**シナリオ価格の合計: $0.16916**

- Data Factory の操作 = **$0.00012**
  - 読み取り／書き込み = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 3\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.16904**
  - アクティビティの実行 = 001\*3 = 0.003 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)
  - 外部パイプラインアクティビティ = $0.000041 （日割の実行時間の10分間です。 $0.00025/時間、 Azure 統合ランタイム)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>データをコピーし動的パラメーターで1時間ごとに変換する

このシナリオでは、1時間ごとのスケジュールでデータを AWS S3 から Azure Blob Storage にコピーし、 Azure Databricks （スクリプト内に動的パラメーターのある）でデータを変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3からコピーされるデータの入力データセット、Azure Storageのデータの出力データセットを含む1つのコピーアクティビティです。
2. 変換スクリプトにパラメーターを動的に渡すための1つの検索アクティビティ。
3. データ変換のための１つの Azure Databricks アクティビティ。
4. パイプラインを 1 時間ごとに実行する 1 つのスケジュールトリガー。

![Scenario3](media/pricing-concepts/scenario3.png)

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 3 つの書き込み/読み取りエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 4 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 3 つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 4 つの監視実行レコードの再試行 (パイプラインの実行用に 1 つ、 アクティビティの実行用に 3 つ) |
| 検索アクティビティ Assumption を実行する: 実行時間 = 1 分 | 1 分間のパイプラインアクティビティの実行 |
| Databricksアクティビティ Assumption を実行する: 実行時間 = 10 分 | 10 分間の外部パイプラインアクティビティの実行 |

**シナリオ価格の合計: $0.17020**

- Data Factory の操作 = **$0.00013**
  - 読み取り／書き込み = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 4\*000005 = $0.00002 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.17007**
  - アクティビティの実行 = 001\*4 = 0.004 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)
  - パイプラインアクティビティ = $0.00003 （日割りの実行時間の1分間です。 $0.002/時間、 Azure 統合ランタイム)
  - 外部パイプラインアクティビティ = $0.000041 （日割の実行時間の10分間です。 $0.00025/時間、 Azure 統合ランタイム)

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>通常の平日にマッピング データ フロー デバッグを使用する

あなたは、データ エンジニアとして、毎日マッピング データ フローを設計、構築、テストする責任があります。 あなたは、朝に ADF UI にログインし、データ フローのデバッグ モードを有効にします。 デバッグ セッションの既定の TTL は、60 分です。 あなたは 1 日 8 時間働いているので、デバッグ セッションは期限切れになりません。 したがって、その日の料金は次のようになります。

**8 (時間) x 8 (コンピューティング最適化コア数) x $0.193 = $12.35**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>マッピング データ フローを使用して BLOB ストア内のデータを変換する

このシナリオでは、時間単位のスケジュールで BLOB ストア内のデータを ADF マッピング データ フローで視覚的に変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. 変換ロジックを含む Data Flow アクティビティ。

2. Azure Storage 上のデータの入力データセット。

3. Azure Storage 上のデータの出力データセット。

4. パイプラインを毎時実行するスケジュールトリガー。

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 2つの読み取り/書き込みエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 2 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 1 つ) |
| Data Flow の前提条件: 実行時間 = 10 分 + TTL 10 分 | 10 \* 16 コアの一般コンピューティング (TTL 10) |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 2つの監視実行レコードの再試行 (パイプラインの実行用に 1 つ、アクティビティの実行用に 1 つ) |

**シナリオ価格の合計: $1.4631**

- Data Factory の操作 = **$0.0001**
  - 読み取り／書き込み = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 監視  = 2\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインのオーケストレーション &amp; 実行 = **$1.463**
  - アクティビティの実行 = 001\*2 = 0.002 [1 実行 = $1/1000 = 0.001]
  - データ フロー アクティビティ = $1.461: 20 分間の時間割り (実行時間 10 分 + TTL 10 分)。 Azure Integration Runtime で $0.274/時間、16 コアの一般コンピューティング

## <a name="next-steps"></a>次のステップ

Azure Data Factory の価格を理解したところで、始めることができます!

- [Azure Data Factory UIを使用してData Factoryを作成する](quickstart-create-data-factory-portal.md)

- [Azure Data Factory の概要](introduction.md)

- [Azure Data Factory の視覚的オーサリング](author-visually.md)
