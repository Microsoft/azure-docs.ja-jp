---
title: Azure Data Factory の Mapping Data Flow のパフォーマンスとチューニング ガイド | Microsoft Docs
description: Mapping Data Flow を使用する場合に、Azure Data Factory でのデータ フローのパフォーマンスに影響する主な要因について説明します。
author: kromerm
ms.topic: conceptual
ms.author: makromer
ms.service: data-factory
ms.date: 05/16/2019
ms.openlocfilehash: 8eb244a0eff1569ac27feae68104db613373463a
ms.sourcegitcommit: 007ee4ac1c64810632754d9db2277663a138f9c4
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/23/2019
ms.locfileid: "69992349"
---
# <a name="mapping-data-flows-performance-and-tuning-guide"></a>Mapping Data Flow のパフォーマンスとチューニング ガイド

[!INCLUDE [notes](../../includes/data-factory-data-flow-preview.md)]

Azure Data Factory の Mapping Data Flow では、大規模なデータ変換の設計、デプロイ、および調整するためのコード不要のブラウザー インターフェイスが提供されます。

> [!NOTE]
> ADF Mapping Data Flow 全般に慣れていない場合は、この記事を読む前に、[データ フローの概要](concepts-data-flow-overview.md)に関するページをご覧ください。
>

> [!NOTE]
> ADF UI から Data Flow の設計およびテストしている場合は、クラスターのウォーム アップを待機することなく、リアルタイムでデータ フローを実行できるように、必ず [デバッグ] スイッチをオンにします。
>

![[デバッグ] ボタン](media/data-flow/debugb1.png "デバッグ")

## <a name="monitor-data-flow-performance"></a>データ フローのパフォーマンスの監視

ブラウザーでマッピング データ フローを設計する際に、変換ごとに下部の設定ウィンドウの [Data Preview]\(データのプレビュー\) タブをクリックして、個々の変換の単体テストを実行できます。 次に行う手順では、パイプライン デザイナーでデータ フローをエンド ツー エンドでテストします。 データ フローの実行アクティビティを追加し、[デバッグ] ボタンを使用して、データ フローのパフォーマンスをテストします。 パイプライン ウィンドウの下部ウィンドウで、[アクション] の下に眼鏡アイコンが表示されます。

![データ フローの監視](media/data-flow/mon002.png "データ フローの監視 2")

そのアイコンをクリックすると、データ フローの実行プランおよび後続のパフォーマンス プロファイルが表示されます。 この情報を使用して、サイズの異なるデータ ソースに対するデータ フローのパフォーマンスを見積もることができます。 全体的なパフォーマンス計算では 1 分間のクラスター ジョブ実行セットアップ時間を想定でき、既定の Azure Integration Runtime を使用している場合は、5 分間のクラスター起動時間も追加する必要が生じる場合があることに注意してください。

![データ フローの監視](media/data-flow/mon003.png "データ フローの監視 3")

## <a name="optimizing-for-azure-sql-database-and-azure-sql-data-warehouse"></a>Azure SQL Database と Azure SQL Data Warehouse の最適化

![ソース部分](media/data-flow/sourcepart3.png "ソース部分")

### <a name="partition-your-source-data"></a>ソース データのパーティション分割

* [最適化] に移動し、[ソース] を選択します。 クエリで特定のテーブル列または型のいずれかを設定します。
* [列] を選択した場合は、パーティション列を選択します。
* また、Azure SQL DB への最大接続数も設定します。 データベースへの並列接続を取得するために高い数値の設定を試すことができます。 ただし、場合によっては、接続の数を制限することでパフォーマンスが高速になることもあります。
* ソース データベース テーブルはパーティション分割する必要はありません。
* ソース変換でデータベース テーブルのパーティション分割スキームに一致するクエリを設定すると、ソース データベース エンジンでパーティション除外を利用できます。
* ソースがまだパーティション分割されていない場合でも、ADF では、ソース変換でユーザーが選択したキーに基づいて、Spark 変換環境でデータのパーティション分割が使用されます。

### <a name="set-batch-size-and-query-on-source"></a>ソースでバッチ サイズとクエリを設定する

![ソース](media/data-flow/source4.png "ソース")

* バッチ サイズを設定することで、行単位ではなくメモリにセットでデータを格納するように ADF に指示します。 これは省略可能な設定で、適切にサイズを設定しないと、コンピューティング ノード上のリソースが不足する場合があります。
* クエリを設定することで、処理のために Data Flow に到着するよりも前に、ソースで直接行をフィルターできます。これにより初期データの取得を高速化することができます。
* クエリを使用する場合は、Azure SQL DB、つまり READ UNCOMMITTED に省略可能なクエリ ヒントを追加することができます。

### <a name="set-isolation-level-on-source-transformation-settings-for-sql-datasets"></a>SQL データセットのソース変換設定に関する分離レベルを設定する

* [コミットされていないものを読み取り] は、ソース変換に関するより高速なクエリ結果を提供します。

![分離レベル](media/data-flow/isolationlevel.png "分離レベル")

### <a name="set-sink-batch-size"></a>シンクのバッチ サイズを設定する

![シンク](media/data-flow/sink4.png "シンク")

* データ フローの行単位の処理を回避するためには、Azure SQL DB のシンクの設定で [バッチ サイズ] を設定します。 これにより、指定したサイズに基づいてデータベースの書き込みをバッチで処理するように ADF に指示します。

### <a name="set-partitioning-options-on-your-sink"></a>シンクでパーティション分割オプションを設定する

* ターゲットの Azure SQL DB テーブルのデータをパーティション分割していない場合でも、[最適化] タブに移動して、パーティション分割を設定します。
* 1 つのノード/パーティションからすべての接続を強制する代わりに、ADF に Spark 実行クラスター上でラウンド ロビン パーティション分割を使用するように指示するだけで、多くの場合、データの読み込みがはるかに高速になります。

### <a name="increase-size-of-your-compute-engine-in-azure-integration-runtime"></a>Azure Integration Runtime でコンピューティング エンジンのサイズを増やす

![新しい IR](media/data-flow/ir-new.png "新しい IR")

* コアの数を増やすと、ノードの数も増えるため、クエリの実行および Azure SQL DB への書き込みの処理能力が向上します。
* ご利用のコンピューティング ノードにさらに多くのリソースを適用するには、[コンピューティング最適化] および [メモリ最適化] のオプションをお試しください。

### <a name="unit-test-and-performance-test-with-debug"></a>デバッグを含む単体テストおよびパフォーマンス テスト

* データ フローを単体テストするときは、[Data Flow Debug]\(データ フローのデバッグ\) ボタンを "ON" に設定します。
* Data Flow デザイナー内で、変換の [Data Preview]\(データのプレビュー\) タブを使用して、変換ロジックの結果を表示します。
* パイプライン デザイン キャンバスに Data Flow アクティビティを配置して、パイプライン デザイナーからデータ フローの単体テストを実行し、[デバッグ] ボタンを使用してテストします。
* デバッグ モードのテストは、Just-In-Time クラスター起動を待機することなく、ウォーミングされたライブ クラスター環境に対して機能します。

### <a name="disable-indexes-on-write"></a>書き込み時にインデックスを無効にする
* シンクからの書き込み先のターゲット テーブルでインデックスを無効にする Data Flow アクティビティの前に、ADF パイプラインのストアド プロシージャ アクティビティを使用します。
* Data Flow アクティビティの後に、これらのインデックスを有効にする別のストアド プロシージャ アクティビティを追加します。

### <a name="increase-the-size-of-your-azure-sql-db"></a>Azure SQL DB のサイズを増やす
* DTU の制限に達したら、ソースおよびシンク Azure SQL DB のサイズ変更をスケジュールしてから、パイプラインを実行してスループットを増やし、Azure スロットルを最小化します。
* パイプラインの実行が完了したら、データベースのサイズを変更して通常のラン レートに戻すことができます。

## <a name="optimizing-for-azure-sql-data-warehouse"></a>Azure SQL Data Warehouse の最適化

### <a name="use-staging-to-load-data-in-bulk-via-polybase"></a>ステージングを使用して、Polybase を介してデータを一括で読み込む

* データ フローの行単位の処理を回避するためには、ADF が Polybase を利用して DW への行単位の挿入を回避できるように、シンク設定で [ステージング] オプションを設定します。 これによって、データを一括で読み込めるように、ADF での Polybase の使用が指示されます。
* ステージングを有効にして、パイプラインからデータ フローのアクティビティを実行する場合は、一括読み込みにおけるステージング データの BLOB ストアの場所を選択する必要があります。

### <a name="increase-the-size-of-your-azure-sql-dw"></a>Azure SQL DW のサイズを増やす

* DWU の制限に達したら、ソースおよびシンク Azure SQL DW のサイズ変更をスケジュールしてから、パイプラインを実行してスループットを増やし、Azure スロットルを最小化します。

* パイプラインの実行が完了したら、データベースのサイズを変更して通常のラン レートに戻すことができます。

## <a name="optimize-for-files"></a>ファイルを最適化する

* ADF で使用するパーティションの数を制御できます。 ソースとシンクの各変換および個々の変換に対してパーティション分割スキームを設定できます。 小さいファイルの場合は、[単一パーティション] を選択すると、小さいファイルをパーティション分割するように Spark に要求するよりも効果的で速い場合があります。
* ソース データに関する十分な情報がない場合は、[ラウンド ロビン] パーティション分割を選択して、パーティションの数を設定できます。
* データを探索して、ハッシュ キーに適した列があることが判明した場合は、ハッシュ パーティション分割のオプションを使用します。
* データ プレビューおよびパイプライン デバッグでデバッグする場合、ファイルベースのソース データセットの制限とサンプリングのサイズは、読み取られる行数ではなく、返される行数にのみ適用されることに注意してください。 これは、ご自身のデバッグの実行のパフォーマンスに影響を与え、場合によってはフローが失敗する可能性があるため、注意が重要です。
* デバッグ クラスターは既定では小規模な単一ノードのクラスターであるため、デバッグには小さい一時ファイルを使用するようにしてください。 [Debug Settings]\(デバッグ設定\) にアクセスし、一時ファイルを使用しているお使いのデータの小さなサブセットをポイントします。

![デバッグ設定](media/data-flow/debugsettings3.png "デバッグ設定")

### <a name="file-naming-options"></a>ファイルの名前付けのオプション

* ADF Mapping Data Flow で変換済みデータを書き込む既定の動作では、BLOB または ADLS のリンクされたサービスを含むデータセットに書き込みます。 名前付きファイルではなく、フォルダーまたはコンテナーを指すように、そのデータセットを設定する必要があります。
* Data Flow は、実行に Azure Databricks Spark を使用します。これは、既定の Spark パーティション分割または明示的に選択したパーティション分割スキームに基づいて、出力が複数のファイルに分割されることを意味します。
* ADF Data Flow では、すべての出力 PART ファイルが単一の出力ファイルにマージされるように "単一ファイルへの出力" を選択するのが一般的な操作です。
* ただし、この操作では、出力を単一のクラスター ノード上の単一のパーティションに減らす必要があります。
* この一般的なオプションを選択する場合は、この点に注意してください。 多数の大きなソース ファイルを単一の出力ファイル パーティションに結合する場合、クラスター ノードのリソースが不足することがあります。
* 計算ノードのリソースを使い果たさないように、パフォーマンスを最適化する既定または明示的なパーティション分割スキームを ADF で保持し、その後、すべての PART ファイルを出力フォルダーから単一の新しいファイルにマージする後続のコピー アクティビティをパイプラインに追加できます。 基本的には、この手法は、変換の動作をファイルのマージから切り離し、"単一ファイルへの出力" を設定するのと同じ結果が得られます。

### <a name="looping-through-file-lists"></a>ファイル リストのループ処理

ほとんどの場合、ADF のデータ フローは、Data Flow のソース変換で複数のファイルを反復処理するパイプラインから実行した方が高速です。 つまりパイプラインで ForEach を使用し、反復のたびにデータ フローの実行を呼び出しながら大量のファイルを反復処理するよりも、Data Flow のソース内でワイルドカードやファイル リストを使用することが推奨されます。 ループ処理を Data Flow 内で行うことにより、Data Flow プロセスの実行速度を上げることができます。

たとえば Blob Storage 内のフォルダーに、処理対象となる 2019 年 7 月の一連のデータ ファイルがある場合、データ フローの実行アクティビティをパイプラインから 1 回呼び出して、次のようにソース内でワイルドカードを使用した方がパフォーマンスが高くなります。

```DateFiles/*_201907*.txt```

パイプラインで BLOB ストアを検索し、ForEach を使用して、その内側でデータ フローの実行アクティビティを使用しながら、一致するすべてのファイルを反復処理するよりも、この方が効率よく実行することができます。

## <a name="next-steps"></a>次の手順

パフォーマンスに関連した他のデータ フローの記事を参照してください。

- [データ フローの [最適化] タブ](concepts-data-flow-optimize-tab.md)
- [Data Flow のアクティビティ](control-flow-execute-data-flow-activity.md)
- [データ フローのパフォーマンスの監視](concepts-data-flow-monitoring.md)
