---
title: Mapping Data Flow のソース変換 - Azure Data Factory | Microsoft Docs
description: Mapping Data Flow のソース変換を設定する方法について説明します。
author: kromerm
ms.author: makromer
ms.service: data-factory
ms.topic: conceptual
ms.date: 09/06/2019
ms.openlocfilehash: c3c24e9dc674ac29c8ca4d0d445cc3f572cda71e
ms.sourcegitcommit: 11265f4ff9f8e727a0cbf2af20a8057f5923ccda
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 10/08/2019
ms.locfileid: "72029217"
---
# <a name="source-transformation-for-mapping-data-flow"></a>Mapping Data Flow のソース変換 



ソース変換は、データ フローのデータ ソースを構成します。 データ フローを設計する際、最初の手順では、常にソース変換を構成します。 ソースを追加するには、データ フローのキャンバスにある **[Add Source]\(ソースの追加\)** ボックスをクリックします。

各データ フローには少なくとも 1 つのソース変換が必要ですが、データ変換を完了するために必要な数だけソースを追加できます。 これらのソースは、結合、参照、または和集合変換を使用して結合できます。

各ソース変換が関連付けられる Data Factory データセットは 1 つだけです。 データセットは、書き込みまたは読み取りを行うデータの形状と場所を定義します。 ファイルベースのデータセットを使用している場合は、ソース内でワイルドカードやファイル リストを使用すると、一度に複数のファイルを操作できます。

## <a name="supported-connectors-in-mapping-data-flow"></a>Mapping Data Flow でサポートされているコネクタ

Mapping Data Flow は、抽出、読み込み、変換 (ELT) のアプローチに従い、すべて Azure に存在する "*ステージング*" データセットを操作します。 現在は、次のデータセットをソース変換で使用できます。
    
* Azure Blob Storage
* Azure Data Lake Storage Gen1
* Azure Data Lake Storage Gen2
* Azure SQL Data Warehouse
* Azure SQL Database

Azure Data Factory は、80 を超えるネイティブ コネクタにアクセスできます。 それらの他のソースからのデータをデータ フローに含めるには、コピー アクティビティを使用して、サポートされているステージング領域のいずれかにそのデータを読み込みます。

## <a name="source-settings"></a>ソースの設定

ソースを追加したら、 **[ソースの設定]** タブを使用して構成します。ここでは、ソースが指すデータセットを選択または作成できます。 また、データのスキーマとサンプリング オプションも選択できます。

![[ソースの設定] タブ](media/data-flow/source1.png "[ソースの設定] タブ")

**スキーマの誤差:** [スキーマの誤差](concepts-data-flow-schema-drift.md)は、データ フロー内の柔軟なスキーマをネイティブに処理するデータ ファクトリの機能であり、列の変更を明示的に定義する必要はありません。

* ソース列が頻繁に変更される場合は、 **[Allow schema drift]\(スキーマの誤差を許可\)** チェック ボックスをオンにします。 この設定により、すべての受信ソース フィールドが変換を通してシンクに流れることができます。

* **[Infer drifted column types]\(誤差のある列の型を推論\)** を選択すると、データ ファクトリは、検出された新しい列ごとにデータ型を検出して定義するよう指示されます。 この機能が無効になっている場合、誤差の列はすべて文字列型になります。

**[スキーマの検証]:** [スキーマの検証] を選択した場合、受信したソース データがデータセットの定義済みスキーマと一致しなければ、データ フローの実行は失敗します。

**[Skip line count]\(スキップ行数\):** この [Skip line count]\(スキップ行数\) フィールドでは、データセットの先頭で無視する行数を指定します。

**[サンプリング]:** ソースからの行数を制限するには、サンプリングを有効にします。 デバッグの目的でソースのデータをテストまたはサンプリングする場合は、この設定を使用します。

ソースが正しく構成されていることを確認するには、デバッグ モードを有効にし、データ プレビューを取り込みます。 詳細については、[デバッグ モード](concepts-data-flow-debug-mode.md)に関するページを参照してください。

> [!NOTE]
> デバッグ モードを有効にすると、データ プレビュー時に、デバッグの設定での行数上限の構成によってソースのサンプリング設定が上書きされます。

## <a name="file-based-source-options"></a>ファイルベースのソース オプション

Azure Blob Storage や Azure Data Lake Storage などのファイルベースのデータセットを使用している場合は、 **[Source Options]\(ソース オプション\)** タブを使用して、ソースでファイルを読み取る方法を管理できます。

![[Source Options]\(ソース オプション\)](media/data-flow/sourceOPtions1.png "[Source Options]\(ソース オプション\)")

**[Wildcard path]\(ワイルドカード パス\)** : ワイルドカード パターンを使用すると、ADF は、単一のソース変換で一致する各フォルダーとファイルをループ処理するよう指示されます。 これは、単一のフロー内の複数のファイルを処理するのに効果的な方法です。 既存のワイルドカード パターンをポイントしたときに表示される + 記号を使って複数のワイルドカード一致パターンを追加します。

ソース コンテナーから、パターンに一致する一連のファイルを選択します。 データセット内で指定できるのはコンテナーのみです。 そのため、ワイルドカード パスには、ルート フォルダーからのフォルダー パスも含める必要があります。

ワイルドカードの例:

* ```*``` - 任意の文字セットを表します。
* ```**``` - ディレクトリの再帰的な入れ子を表します。
* ```?``` - 1 文字を置き換えます。
* ```[]``` - 角カッコ内の文字のいずれか 1 つと一致します。

* ```/data/sales/**/*.csv``` - /data/sales の下のすべての csv ファイルを取得します。
* ```/data/sales/20??/**``` - 20 世紀のすべてのファイルを取得します。
* ```/data/sales/2004/*/12/[XY]1?.csv``` - 前に 2 桁の数字が付いた X または Y で始まる、2004 年 12 月のすべての csv ファイルを取得します。

**[Partition root path]\(パーティションのルート パス\):** ```key=value``` 形式 (例: year=2019) のファイル ソース内のフォルダーをパーティション分割した場合、そのパーティション フォルダー ツリーの最上位をデータ フロー データ ストリーム内の列名に割り当てることができます。

最初に、ワイルドカードを設定して、パーティション分割されたフォルダーと読み取るリーフ ファイルのすべてのパスを含めます。

![パーティション ソース ファイルの設定](media/data-flow/partfile2.png "パーティション ファイルの設定")

パーティションのルート パス設定を使用して、フォルダー構造の最上位レベルを定義します。 データ プレビューを使用してデータの内容を表示すると、ADF によって、各フォルダー レベルで見つかった解決済みのパーティションが追加されることがわかります。

![パーティションのルート パス](media/data-flow/partfile1.png "パーティションのルート パスのプレビュー")

**[ファイルの一覧]:** これはファイル セットです。 処理する相対パス ファイルの一覧を含むテキスト ファイルを作成します。 このテキスト ファイルをポイントします。

**[Column to store file name]\(ファイル名を格納する列\):** ソース ファイルの名前をデータの列に格納します。 ファイル名文字列を格納するための新しい列名をここに入力します。

**[After completion]\(完了後\):** データ フローの実行後にソース ファイルに何もしないか、ソース ファイルを削除するか、またはソース ファイルを移動することを選択します。 移動のパスは相対パスです。

後処理でソース ファイルを別の場所に移動するには、まず、ファイル操作の "移動" を選択します。 次に、"移動元" ディレクトリを設定します。 パスにワイルドカードを使用していない場合、"移動元" 設定はソース フォルダーと同じフォルダーになります。

ワイルドカードを含むソース パスがある場合、構文は次のようになります。

```/data/sales/20??/**/*.csv```

"移動元" は次のように指定できます。

```/data/sales```

"移動先" は次のように指定できます。

```/backup/priorSales```

この場合、ソースとして指定された /data/sales の下のすべてのファイルは /backup/priorSales に移動されます。

> [!NOTE]
> ファイル操作は、パイプライン内のデータ フローの実行アクティビティを使用するパイプライン実行 (パイプラインのデバッグまたは実行) からデータ フローを開始する場合にのみ実行されます。 データ フロー デバッグ モードでは、ファイル操作は実行*されません*。

**[Filter by last modified]\(最終更新日時でフィルター処理\):** 最終更新日時の範囲を指定することで、処理するファイルをフィルター処理できます。 日時はすべて UTC 形式です。 

### <a name="add-dynamic-content"></a>動的なコンテンツの追加

[Mapping Data Flow の変換式言語](data-flow-expression-functions.md)を使用して、ソースの設定すべてを式として指定できます。 動的なコンテンツを追加するには、設定パネルのフィールドの内部をクリックまたはポイントします。 **[動的なコンテンツの追加]** のハイパーリンクをクリックします。 これにより、式ビルダーが起動します。ここでは、式、静的なリテラル値、またはパラメーターを使用して値を動的に設定できます。

![パラメーター](media/data-flow/params6.png "パラメーター")

## <a name="sql-source-options"></a>SQL のソース オプション

ソースが SQL Database または SQL Data Warehouse に存在する場合は、 **[Source Options]\(ソース オプション\)** タブで他にも SQL 固有の設定を使用できます。 

**[入力]:** テーブルにあるソースを指す (```Select * from <table-name>``` に相当) かカスタム SQL クエリを入力するかを選択します。

**Query**: [入力] フィールドで [クエリ] を選択した場合は、ソースに対する SQL クエリを入力します。 この設定により、データセットで選択したすべてのテーブルがオーバーライドされます。 ここでは **Order By** 句はサポートされていませんが、完全な SELECT FROM ステートメントを設定することができます。 ユーザー定義のテーブル関数を使用することもできます。 **select * from udfGetData()** は、テーブルを返す SQL の UDF です。 このクエリでは、お使いのデータ フローで使用できるソース テーブルが生成されます。

**バッチ サイズ**: 大量データを読み取りにまとめるバッチ サイズを入力します。

**分離レベル**: Mapping Data Flow での SQL ソースの既定値は [コミットされていないものを読み取り] です。 ここで分離レベルを次のいずれかの値に変更できます。
* コミットされたものを読み取り
* コミットされていないものを読み取り
* 反復可能読み取り
* シリアル化可能
* なし (分離レベルを無視)

![分離レベル](media/data-flow/isolationlevel.png "分離レベル")

## <a name="projection"></a>プロジェクション

データセット内のスキーマと同様に、ソース内のプロジェクションでは、ソース データのデータの列、型、および形式が定義されます。 SQL や Parquet など、ほとんどのデータセットの種類では、ソース内のプロジェクトは、データセットで定義されたスキーマを反映するように固定されています。 ソース ファイルが厳密に型指定されていない場合 (たとえば、Parquet ファイルでなく csv ファイル)、このソース変換では各フィールドのデータ型を定義できます。

![[Projection]\(プロジェクション\) タブでの設定](media/data-flow/source3.png "プロジェクション")

テキスト ファイルが定義済みのスキーマを含まない場合は、Data Factory がデータ型をサンプリングして推論するように、 **[データ型の検出]** を選択します。 **[Define default format]\(既定の形式の定義\)** を選択して、既定のデータ形式を自動検出します。 

列のデータ型は、下流の派生列変換で変更できます。 選択変換を使用して、列の名前を変更します。

## <a name="optimize-the-source-transformation"></a>ソース変換を最適化する

ソース変換の **[最適化]** タブに、 **[ソース]** というパーティションの種類が表示されることがあります。 このオプションは、ソースが Azure SQL Database の場合にのみ使用できます。 これは、Data Factory が SQL Database ソースに対して大規模なクエリを実行するために、接続を並列化しようとするためです。

![[ソース] パーティション設定](media/data-flow/sourcepart3.png "パーティション分割")

SQL Database ソースのデータをパーティション分割する必要はありませんが、パーティションは大規模なクエリの場合に便利です。 列やクエリに基づいてパーティションを作成できます。

### <a name="use-a-column-to-partition-data"></a>列を使用してデータをパーティション分割する

ソース テーブルから、パーティション分割する列を選択します。 パーティション数も設定します。

### <a name="use-a-query-to-partition-data"></a>クエリを使用してデータをパーティション分割する

クエリに基づいて接続をパーティション分割することを選択できます。 WHERE 述語の内容を入力します。 たとえば、「year > 1980」と入力します。

Mapping Data Flow 内での最適化の詳細については、[[最適化] タブ](concepts-data-flow-overview.md#optimize)に関するページを参照してください。

## <a name="next-steps"></a>次の手順

[派生列変換](data-flow-derived-column.md)と[選択変換](data-flow-select.md)の作成を開始します。
