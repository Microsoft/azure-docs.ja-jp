---
title: Azure Data Factory の Mapping Data Flow 機能のソース変換を設定する
description: Mapping Data Flow のソース変換を設定する方法について説明します。
author: kromerm
ms.author: makromer
ms.service: data-factory
ms.topic: conceptual
ms.date: 02/12/2019
ms.openlocfilehash: 4f77eafd3309d7c1d679c126b1a5eb1ff0e9a28d
ms.sourcegitcommit: ac1cfe497341429cf62eb934e87f3b5f3c79948e
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/01/2019
ms.locfileid: "67490096"
---
# <a name="source-transformation-for-mapping-data-flow"></a>Mapping Data Flow のソース変換 

[!INCLUDE [notes](../../includes/data-factory-data-flow-preview.md)]

ソース変換は、データ フローのデータ ソースを構成します。 データ フローには複数のソース変換を含めることができます。 データ フローを設計するときは、常にソース変換から開始してください。

どのデータ フローにも、少なくとも 1 つのソース変換が必要です。 データ変換を完了ために必要な数のソースを追加します。 これらのソースは、結合変換または和集合変換を使用して結合できます。

> [!NOTE]
> データ フローをデバッグするとき、データはサンプリング設定またはデバッグ ソース制限を使用してソースから読み取られます。 データをシンクに書き込むには、パイプライン データ フロー アクティビティからデータ フローを実行する必要があります。 

![[Source Settings]\(ソース設定\) タブのソース変換オプション](media/data-flow/source.png "ソース")

データ フロー ソース変換を、厳密に 1 つの Data Factory データセットに関連付けます。 データセットは、書き込みまたは読み取りを行うデータの形状と場所を定義します。 ソース内でワイルドカードやファイル一覧を使用すると、同時に複数のファイルを操作できます。

## <a name="data-flow-staging-areas"></a>データ フローのステージング領域

データ フローは、すべてが Azure に存在する*ステージング* データセットを操作します。 データを変換するときは、これらのステージング用のデータセットを使用します。 

Data Factory は、約 80 個のネイティブ コネクタにアクセスできます。 それらの他のソースからのデータをデータ フローに含めるには、コピー アクティビティ ツールを使用して、そのデータをデータ フロー データセットのステージング領域のいずれかにステージングします。

## <a name="options"></a>オプション

データのスキーマとサンプリング オプションを選択します。

### <a name="allow-schema-drift"></a>[Allow Schema Drift] (スキーマの誤差を許可)
ソース列が頻繁に変更される場合は、 **[Allow Schema Drift]\(スキーマの誤差を許可\)** を選択します。 この設定により、すべての受信ソース フィールドが変換を通してシンクに流れることができます。

### <a name="validate-schema"></a>[スキーマの検証]

ソース データの受信バージョンが定義済みのスキーマに一致しない場合、データ フローの実行は失敗します。

![パブリック ソース設定で、[スキーマの検証]、[Allow Schema Drift]\(スキーマの誤差を許可\)、および [サンプリング] のオプションを表示](media/data-flow/source1.png "パブリック ソース 1")

### <a name="sample-the-data"></a>データをサンプリングする
ソースからの行数を制限するには、**サンプリング**を有効にします。 デバッグの目的でソースのデータをテストまたはサンプリングする場合は、この設定を使用します。

## <a name="define-schema"></a>スキーマを定義する

ソース ファイルが厳密に型指定されていない場合 (たとえば、Parquet ファイルでなくフラット ファイル)、このソース変換で各フィールドのデータ型を定義します。  

![[Define schema]\(スキーマを定義する\) タブのソース変換設定](media/data-flow/source2.png "ソース 2")

列名は選択変換で後で変更できます。 派生列変換を使用してデータ型を変更します。 厳密に型指定されたソースの場合は、後で選択変換でデータ型を変更できます。 

![選択変換でのデータ型](media/data-flow/source003.png "データ型")

### <a name="optimize-the-source-transformation"></a>ソース変換を最適化する

ソース変換の **[最適化]** タブに、 **[ソース]** というパーティションの種類が表示されることがあります。 このオプションは、ソースが Azure SQL Database の場合にのみ使用できます。 これは、Data Factory が SQL Database ソースに対して大規模なクエリを実行するために、接続を並列化しようとするためです。

![[ソース] パーティション設定](media/data-flow/sourcepart3.png "パーティション分割")

SQL Database ソースのデータをパーティション分割する必要はありませんが、パーティションは大規模なクエリの場合に便利です。 列やクエリに基づいてパーティションを作成できます。

### <a name="use-a-column-to-partition-data"></a>列を使用してデータをパーティション分割する

ソース テーブルから、パーティション分割する列を選択します。 パーティション数も設定します。

### <a name="use-a-query-to-partition-data"></a>クエリを使用してデータをパーティション分割する

クエリに基づいて接続をパーティション分割することを選択できます。 WHERE 述語の内容を入力するだけです。 たとえば、「year > 1980」と入力します。

## <a name="source-file-management"></a>ソース ファイルの管理

ソース内のファイルを管理するための設定を選択します。 

![新しいソース設定](media/data-flow/source2.png "新しい設定")

* **ワイルドカードのパス**:ソース フォルダーから、パターンに一致する一連のファイルを選択します。 この設定により、データセット定義内のすべてのファイルがオーバーライドされます。

ワイルドカードの例:

* ```*``` - 任意の文字セットを表します。
* ```**``` - ディレクトリの再帰的な入れ子を表します。
* ```?``` - 1 文字を置き換えます。
* ```[]``` - 角カッコ内の文字のいずれか 1 つと一致します。

* ```/data/sales/**/*.csv``` - /data/sales の下のすべての csv ファイルを取得します。
* ```/data/sales/20??/**``` - 20 世紀のすべてのファイルを取得します。
* ```/data/sales/2004/*/12/[XY]1?.csv``` - 前に 2 桁の数字が付いた X または Y で始まる、2004 年 12 月のすべての csv ファイルを取得します。

コンテナーは、データセット内で指定する必要があります。 そのため、ワイルドカード パスには、ルート フォルダーからのフォルダー パスも含める必要があります。

* **ファイルの一覧**:これはファイル セットです。 処理する相対パス ファイルの一覧を含むテキスト ファイルを作成します。 このテキスト ファイルをポイントします。
* **ファイル名を格納する列**:ソース ファイルの名前をデータの列に格納します。 ファイル名文字列を格納するには、ここに新しい名前を入力します。
* **完了した後**:データ フローの実行後にソース ファイルに何もしないか、ソース ファイルを削除するか、またはソース ファイルを移動することを選択します。 移動のパスは相対パスです。

後処理でソース ファイルを別の場所に移動するには、まず、ファイル操作の "移動" を選択します。 次に、"移動元" ディレクトリを設定します。 パスにワイルドカードを使用していない場合、"移動元" 設定はソース フォルダーと同じフォルダーになります。

次のように、ワイルドカードが含まれたソース パスを使用する場合、

```/data/sales/20??/**/*.csv```

"移動元" は次のように指定できます。

```/data/sales```

"移動先" は次のように指定できます。

```/backup/priorSales```

この場合、ソースとして指定された /data/sales の下のすべてのサブディレクトリは、/backup/priorSales を基準として移動されます。

### <a name="sql-datasets"></a>SQL データセット

ソースが SQL Database または SQL Data Warehouse 内にある場合、ソース ファイル管理のための追加オプションがあります。

* **Query**: ソースに対する SQL クエリを入力します。 この設定により、データセットで選択したすべてのテーブルがオーバーライドされます。 なお、ここで **Order By** 句はサポートされていませんが、完全な SELECT FROM ステートメントを設定することができます。 ユーザー定義のテーブル関数を使用することもできます。 **select * from udfGetData()** は、テーブルを返す SQL の UDF です。 このクエリでは、お使いのデータ フローで使用できるソース テーブルが生成されます。
* **バッチ サイズ**: 大量データを読み取りにまとめるバッチ サイズを入力します。
* **分離レベル**: ADF の Mapping Data Flow での SQL ソースの既定値は "コミットされていないものを読み取り" です。 ここで分離レベルを次のいずれかの値に変更できます。
* コミットされたものを読み取り
* コミットされていないものを読み取り
* 反復可能読み取り
* シリアル化可能
* なし (分離レベルを無視)

![分離レベル](media/data-flow/isolationlevel.png "分離レベル")

> [!NOTE]
> ファイル操作は、パイプライン内のデータ フローの実行アクティビティを使用するパイプライン実行 (パイプラインのデバッグまたは実行) からデータ フローを開始する場合にのみ実行されます。 データ フロー デバッグ モードでは、ファイル操作は実行*されません*。

### <a name="projection"></a>プロジェクション

データセット内のスキーマと同様に、ソース内のプロジェクションでは、ソース データのデータの列、型、および形式が定義されます。 

![[Projection]\(プロジェクション\) タブでの設定](media/data-flow/source3.png "プロジェクション")

テキスト ファイルが定義済みのスキーマを含まない場合は、Data Factory がデータ型をサンプリングして推論するように、 **[データ型の検出]** を選択します。 **[Define default format]\(既定の形式の定義\)** を選択して、既定のデータ形式を自動検出します。 

列のデータ型は、以降の派生列変換で変更できます。 選択変換を使用して、列の名前を変更します。

![既定のデータ形式の設定](media/data-flow/source2.png "既定の形式")

### <a name="add-dynamic-content"></a>動的なコンテンツの追加

設定パネルのフィールド内をクリックすると、[動的なコンテンツの追加] というハイパーリンクが表示されます。 ここをクリックすると、式ビルダーが起動します。 式ビルダーでは、式、静的リテラル値、またはパラメーターを使用して動的に設定する値を設定できます。

![パラメーター](media/data-flow/params6.png "パラメーター")

## <a name="next-steps"></a>次の手順

[派生列変換](data-flow-derived-column.md)と[選択変換](data-flow-select.md)の作成を開始します。
