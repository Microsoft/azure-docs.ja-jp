---
title: メディア グラフの概念 - Azure
description: メディア グラフを使用すると、メディアのキャプチャ元、処理方法、および結果の配信先を定義できます。 この記事では、メディア グラフの概念の詳細について説明します。
ms.topic: conceptual
ms.date: 05/01/2020
ms.openlocfilehash: ad23acbbbdd0c15e92e471ee22a229470a8a3a75
ms.sourcegitcommit: 32e0fedb80b5a5ed0d2336cea18c3ec3b5015ca1
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/30/2021
ms.locfileid: "105557668"
---
# <a name="media-graph"></a>メディア グラフ

## <a name="suggested-pre-reading"></a>推奨される事前読み取り

* [IoT Edge の Live Video Analytics (プレビュー)](overview.md)
* [IoT Edge の Live Video Analytics の用語](terminology.md)

## <a name="overview"></a>概要

メディア グラフを使用すると、メディアのキャプチャ元、処理方法、および結果の配信先を定義できます。 これを行うには、必要な方法でコンポーネントまたはノードを接続します。 メディア グラフを図で表わすと、次の図のようになります。  

> [!div class="mx-imgBorder"]
> :::image type="content" source="./media/media-graph/media-graph.svg" alt-text="メディア グラフ":::

IoT Edge の Live Video Analytics では、さまざまな種類のソース、プロセッサ、およびシンクがサポートされています。

* **ソース ノード** は、メディア グラフへのメディアのキャプチャを有効にします。 このコンテキストのメディアは、概念的には、オーディオ ストリーム、ビデオ ストリーム、データ ストリーム、またはオーディオ、ビデオ、データを 1 つのストリームに結合したストリームを指します。
* **プロセッサ ノード** は、メディア グラフ内のメディアの処理を有効にします。
* **シンク ノード** は、メディア グラフの外部にあるサービスとアプリに処理結果を配信できます。

## <a name="media-graph-topologies-and-instances"></a>メディア グラフのトポロジとインスタンス 

IoT Edge の Live Video Analytics では、"グラフ トポロジ" と "グラフ インスタンス" の 2 つの概念によってメディア グラフを管理できます。 グラフ トポロジを使用すると、値のプレースホルダーとしてパラメーターを使用して、グラフのブループリントを定義できます。 トポロジは、メディア グラフでどのノードを使用するか、およびそのノードがメディア グラフでどのように接続されるかを定義します。 たとえば、カメラからフィードを記録する場合は、ビデオを受信するソース ノードとビデオを書き込むシンク ノードを含むグラフが必要になります。

トポロジ内のパラメーターの値は、トポロジを参照するグラフ インスタンスを作成するときに指定します。 これにより、同じトポロジを参照しつつ、トポロジで指定されているパラメーターの値が異なる複数のインスタンスを作成できます。 上記の例では、パラメーターを使用して、カメラの IP アドレスと、録画されたビデオの名前を表すこともできます。 そのトポロジを使用して多数のグラフ インスタンスを作成できます。たとえば、それぞれに特定の IP アドレスと特定の名前が付いている、建物内のカメラごとに 1 つのインスタンスを作成できます。

## <a name="media-graph-states"></a>メディア グラフの状態  

グラフ トポロジとグラフ インスタンスのライフサイクルを次の状態ダイアグラムに示します。

> [!div class="mx-imgBorder"]
> :::image type="content" source="./media/media-graph/graph-topology-lifecycle.svg" alt-text="グラフ トポロジとグラフ インスタンスのライフサイクル":::

まず、[グラフ トポロジの作成](direct-methods.md#graphtopologyset)から始めます。 次に、このトポロジを使用して処理するライブ ビデオ フィードごとに、[グラフ インスタンスを作成](direct-methods.md#graphinstanceset)します。 

このグラフ インスタンスは、`Inactive` (アイドル) 状態になります。

ライブ ビデオ フィードをグラフ インスタンスに送信する準備ができたら、[アクティブ化](direct-methods.md#graphinstanceactivate)します。 グラフ インスタンスは一時的に `Activating` 状態に移行し、成功した場合は `Active` 状態に移行します。 `Active` 状態では、メディアが処理されます (グラフ インスタンスで入力データが受け取られた場合)。

> [!NOTE]
>  グラフ インスタンスは、データが流れていない間もアクティブにすることができます (たとえば、カメラがオフラインになった場合など)。
> Azure サブスクリプションは、グラフ インスタンスがアクティブ状態である時に対して課金されます。

他のライブ ビデオ フィードを処理する場合は、同じトポロジに対して他のグラフ インスタンスを作成してアクティブ化するプロセスを繰り返します。

ライブ ビデオ フィードの処理が完了したら、グラフ インスタンスを[非アクティブ化](direct-methods.md#graphinstancedeactivate)します。 グラフ インスタンスは、一時的に `Deactivating` 状態に移行し、持っているデータがフラッシュされた後、`Inactive` 状態に戻ります。

グラフ インスタンスが `Inactive` 状態にある場合にのみ、そのグラフ インスタンスを[削除](direct-methods.md#graphinstancedelete)できます。

特定のグラフ トポロジを参照するすべてのグラフ インスタンスが削除された後、[そのグラフ トポロジを削除](direct-methods.md#graphtopologydelete)することができます。


## <a name="sources-processors-and-sinks"></a>ソース、プロセッサ、およびシンク  

IoT Edge の Live Video Analytics では、メディア グラフ内の次の種類のノードがサポートされています。

### <a name="sources"></a>変換元 

#### <a name="rtsp-source"></a>RTSP ソース 

RTSP ソース ノードを使用すると、[RTSP](https://tools.ietf.org/html/rfc2326) サーバーからメディアを取り込むことができます。 監視と IP ベースのカメラは、RTSP (リアルタイム ストリーミング プロトコル) と呼ばれるプロトコルでデータを送信します。これは、携帯電話やビデオ カメラなどの他の種類のデバイスとは異なります。 このプロトコルは、サーバー (カメラ) とクライアントの間のメディア セッションを確立して制御するために使用されます。 メディア グラフの RTSP ソース ノードはクライアントとして機能し、RTSP サーバーとのセッションを確立できます。 ほとんどの [IP カメラ](https://en.wikipedia.org/wiki/IP_camera)などの多くのデバイスには、組み込みの RTSP サーバーがあります。 [ONVIF](https://www.onvif.org/) は、[プロファイル G、S & T](https://www.onvif.org/wp-content/uploads/2019/12/ONVIF_Profile_Feature_overview_v2-3.pdf) 準拠デバイスの定義で RTSP をサポートするように指示します。 RTSP ソース ノードでは、認証された接続を有効にするための資格情報と共に RTSP URL を指定する必要があります。

#### <a name="iot-hub-message-source"></a>IoT Hub メッセージのソース 

他の [IoT Edge モジュール](../../iot-fundamentals/iot-glossary.md#iot-edge)と同様に、IoT Edge モジュールの Live Video Analytics では、[IoT Edge ハブ](../../iot-fundamentals/iot-glossary.md#iot-edge-hub)を介してメッセージを受信できます。 これらのメッセージは、他のモジュール、またはエッジ デバイスで実行されているアプリ、またはクラウドから送信できます。 このようなメッセージは、モジュールの[名前付きの入力](../../iot-edge/module-composition.md#sink)に配信 (ルーティング) されます。 IoT Hub メッセージのソース ノードを使用すると、このようなメッセージをメディア グラフに配信できます。 これらのメッセージまたは信号は、通常、信号ゲートをアクティブ化するために、メディア グラフの内部で使用できます (後述する[シグナル ゲート](#signal-gate-processor)を参照してください)。 

たとえば、ドアが開かれたときにメッセージを生成する IoT Edge モジュールを作成できます。 そのモジュールからのメッセージを IoT Edge hub にルーティングし、このハブから、メディア グラフの IoT hub メッセージ ソースにルーティングできます。 メディア グラフ内では、IoT hub のメッセージ ソースは、イベントをシグナル ゲート プロセッサに渡すことができます。これにより、RTSP ソースからファイルへのビデオの記録が有効になります。 

### <a name="processors"></a>[プロセッサ]  

#### <a name="motion-detection-processor"></a>モーション検出プロセッサ 

モーション検出プロセッサ ノードを使用すると、ライブ ビデオのモーションを検出できます。 受信したビデオ フレームを調べ、ビデオに動きがあるかどうかを判断します。 モーションが検出されると、そのビデオ フレームをダウンストリーム コンポーネントに渡し、イベントを出力します。 モーション検出プロセッサ ノードを (他のノードと組み合わせて) 使用すると、モーションが検出されたときに受信ビデオの記録をトリガーできます。

#### <a name="frame-rate-filter-processor"></a>フレーム レート フィルター プロセッサ  

フレーム レート フィルター プロセッサ ノードを使用すると、受信したビデオ ストリームのフレームを指定したレートでサンプリングできます。 これにより、さらに処理するためにダウンストリーム コンポーネント (HTTP 拡張プロセッサ ノードなど) に送信されるフレームの数を減らすことができます。
>[!WARNING]
> このプロセッサは IoT Edge モジュールの Live Video Analytics の最新リリースで **非推奨** となっています。 フレーム レート管理は、グラフ拡張プロセッサ自体でサポートされるようになりました。

#### <a name="http-extension-processor"></a>HTTP 拡張プロセッサ

HTTP 拡張プロセッサ ノードを使用すると、独自の IoT Edge モジュールをメディア グラフに接続できます。 このノードは、デコードされたビデオ フレームを入力として受け取り、そのようなフレームをモジュールによって公開される HTTP REST エンドポイントにリレーします。 このノードは、必要に応じて、REST エンドポイントで認証することができます。 また、ノードには、ビデオ フレームを REST エンドポイントにリレーする前に、スケールおよびエンコードするための組み込みのイメージ フォーマッタがあります。 スケーラーには、画像の縦横比を維持、埋め込み、または拡張するオプションがあります。 イメージ エンコーダーでは、JPEG、PNG、BMP 形式がサポートされています。 プロセッサの詳細については、[こちら](media-graph-extension-concept.md#http-extension-processor)をご覧ください。

#### <a name="grpc-extension-processor"></a>gRPC 拡張プロセッサ

gRPC 拡張プロセッサ ノードは、デコードされたビデオ フレームを入力として受け取り、そのようなフレームをモジュールによって公開される [gRPC](terminology.md#grpc) エンドポイントにリレーします。 ノードでは、[共有メモリ](https://en.wikipedia.org/wiki/Shared_memory)を使用したデータの転送や、gRPC メッセージの本文へのコンテンツの直接埋め込みをサポートします。 また、ノードには、ビデオ フレームを gRPC エンドポイントにリレーする前に、スケールおよびエンコードするための組み込みのイメージ フォーマッタがあります。 スケーラーには、画像の縦横比を維持、埋め込み、または拡張するオプションがあります。 イメージ エンコーダーでは、jpeg、png、bmp 形式がサポートされています。 プロセッサの詳細については、[こちら](media-graph-extension-concept.md#grpc-extension-processor)をご覧ください。

#### <a name="signal-gate-processor"></a>シグナル ゲート プロセッサ  

シグナル ゲート プロセッサ ノードを使用すると、あるノードから別のノードにメディアを条件付きで転送できます。 また、メディアとイベントの同期を可能にするバッファーとしても機能します。 一般的なユース ケースでは、RTSP ソース ノードとアセット シンク ノードの間にシグナル ゲート プロセッサ ノードを挿入し、モーション検出プロセッサ ノードの出力を使用してゲートをトリガーします。 このようなメディア グラフでは、モーションが検出された場合にのみビデオが録画されます。

### <a name="sinks"></a>シンク  

#### <a name="asset-sink"></a>アセット シンク  

アセット シンク ノードを使用すると、メディア (ビデオやオーディオ) データを Azure Media Services アセットに書き込むことができます。 1 つのメディア グラフに存在できるアセット シンク ノードは 1 つだけです。 アセットの詳細と、メディアの記録と再生における役割については、[アセット](terminology.md#asset)に関するセクションを参照してください。 このノードのプロパティの使用方法の詳細については、[継続的なビデオ記録](continuous-video-recording-concept.md)に関する記事も参照してください。

#### <a name="file-sink"></a>ファイル シンク  

ファイル シンク ノードを使用すると、メディア (ビデオやオーディオ) データを IoT Edge デバイスのローカル ファイル システム上の場所に書き込むことができます。 メディア グラフには 1 つのファイル シンク ノードしか存在できません。また、シグナル ゲート プロセッサ ノードのダウンストリームにある必要があります。 これにより、出力ファイルの長さが、シグナル ゲート プロセッサのノード プロパティで指定された値に制限されます。 エッジ デバイスのディスク領域が不足しないようにするために、IoT Edge 上の Live Video Analytics モジュールがデータを保存するために使用できる最大サイズを設定することもできます。  
> [!NOTE]
ファイル シンクがいっぱいになると、IoT Edge モジュールの Live Video Analytics によって最も古いデータの削除が開始され、新しいデータで置き換えられます。
#### <a name="iot-hub-message-sink"></a>IoT Hub メッセージ シンク  

IoT Hub メッセージ シンク ノードを使用すると IoT Edge ハブにイベントを発行できます。 その後、IoT Edge ハブは、エッジ デバイス上の他のモジュールやアプリ、またはクラウド内の IoT Hub にデータをルーティングすることができます (配置マニフェストで指定されているルートごと)。 IoT Hub メッセージ シンク ノードは、モーション検出プロセッサ ノードなどのアップストリーム プロセッサから、または HTTP 拡張プロセッサ ノードを介して外部の推論サービスからイベントを受け取ることができます。

## <a name="rules-on-the-use-of-nodes"></a>ノードの使用に関する規則

メディア グラフ内でさまざまなノードを使用する上で適用される他の規則については、「[グラフ トポロジに関する制限](quotas-limitations.md#limitations-on-graph-topologies-at-preview)」を参照してください。

## <a name="scenarios"></a>シナリオ

上記で定義したソース、プロセッサ、およびシンクを組み合わせて使用することで、ライブ ビデオの分析に関連するさまざまなシナリオでメディア グラフを作成できます。 シナリオの例を次に示します。

* [継続的なビデオ記録](continuous-video-recording-concept.md)
* [イベントベースのビデオ記録](event-based-video-recording-concept.md)
* [ビデオ記録を行わない Live Video Analytics](analyze-live-video-concept.md)

## <a name="next-steps"></a>次のステップ

ライブ ビデオ フィードでモーション検出を実行する方法については、「[クイック スタート:ご自分のモデルを使用して Live Video Analytics を実行する](use-your-model-quickstart.md)」を参照してください。
