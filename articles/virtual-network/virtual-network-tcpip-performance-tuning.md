---
title: Azure VM の TCP/IP パフォーマンス チューニング | Microsoft Docs
description: 各種の一般的な TCP/IP パフォーマンス チューニング手法、およびその手法と Azure VM との関係について説明します。
services: virtual-network
documentationcenter: na
author:
- rimayber
- dgoddard
- stegag
- steveesp
- minale
- btalb
- prachank
manager: paragk
editor: ''
ms.assetid: ''
ms.service: virtual-network
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: infrastructure-services
ms.date: 3/30/2019
ms.author:
- rimayber
- dgoddard
- stegag
- steveesp
- minale
- btalb
- prachank
ms.openlocfilehash: 664c8b659152a370d7fb31907b6cdbcd414dce31
ms.sourcegitcommit: 9f4eb5a3758f8a1a6a58c33c2806fa2986f702cb
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 04/03/2019
ms.locfileid: "58905104"
---
# <a name="tcpip-performance-tuning-for-azure-vms"></a>Azure VM の TCP/IP パフォーマンス チューニング

この記事の目的は、一般的な TCP/IP パフォーマンス チューニング手法と、Microsoft Azure 上で実行されている仮想マシンでのその手法の考慮事項について説明することです。 まず概念の基本を理解してから、それらのチューニング方法について議論することが重要です。

## <a name="common-tcpip-tuning-techniques"></a>一般的な TCP/IP チューニング手法

### <a name="mtu-fragmentation-and-large-send-offload-lso"></a>MTU、断片化、Large Send Offload (LSO)

#### <a name="explanation-of-mtu"></a>MTU の説明

最大転送単位 (MTU) は、ネットワーク インターフェイスを介して送信できる、バイト単位で指定された最大サイズのフレーム (パケット) です。 MTU は構成可能な設定であり、Azure VM 上で使用される既定の MTU と、ほとんどのネットワーク デバイス上のグローバルな既定の設定は、1,500 バイトです。

#### <a name="explanation-of-fragmentation"></a>断片化の説明

断片化は、ネットワーク インターフェイスの MTU を超えるパケットが送信されるときに発生します。 TCP/IP スタックにより、パケットは、インターフェイスの MTU に準拠したより小さい部分 (フラグメント) に分割されます。 断片化は IP レイヤーで行われ、基になるプロトコル (TCP など) には依存しません。 MTU が 1,500 であるネットワーク インターフェイス上に 2,000 バイトのパケットが送信されると、そのパケットは 1,500 バイトのパケット 1 つと 500 バイトのパケット 1 つに分割されます。

ソースと宛先の間のパス上にあるネットワーク デバイスには、MTU を超えるパケットを削除するか、パケットをより小さい部分に断片化するオプションがあります。

#### <a name="the-dont-fragment-df-bit-in-an-ip-packet"></a>IP パケット内の "断片化禁止 (DF)" ビット

断片化禁止ビットは、IP プロトコル ヘッダー内のフラグです。 DF ビットが設定されている場合、送信者と受信者の間のパス上にある中間ネットワーク デバイスでパケットを断片化してはならないことを示します。 このビットを設定する理由は数多くあります (一例として、以下のパス検出のセクションをご覧ください)。 断片化禁止ビットが設定されたパケットがネットワーク デバイスによって受信され、そのパケットがデバイスのインターフェイスの MTU を超えている場合、デバイスの標準的な動作は、パケットを削除し、"ICMP Fragmentation Needed" (ICMP 断片化が必要) パケットをパケットの元のソースに送信することです。

#### <a name="performance-implications-of-fragmentation"></a>断片化がパフォーマンスに与える影響

断片化は、パフォーマンスに悪影響を与えることがあります。 パフォーマンスへの影響の主な理由の 1 つは、パケットの断片と再構築が CPU/メモリに与える影響です。 ネットワーク デバイスがパケットの断片化を必要とする場合、断片化を実行するための CPU/メモリ リソースを割り当てる必要があります。 パケットの再構築時にも同じことを行う必要があります。 ネットワーク デバイスでは、元のパケットに再構築できるように、すべてのフラグメントが受信されるまでそれらを格納しておく必要があります。 この断片化/再構築のプロセスは、断片化/再構築プロセスによる待ち時間の原因にもなる可能性があります。

断片化について考えられるパフォーマンスへの他の悪影響は、断片化されたパケットが順不同で到着する可能性があることです。 パケットが順不同だと、特定の種類のネットワーク デバイスでは順序が正しくないパケットが削除されることがあります。これにより、パケット全体の再送信が必要になります。 フラグメントの削除の一般的なシナリオとして、ネットワーク ファイアウォールなどのセキュリティ デバイスや、ネットワーク デバイスの受信バッファーが使い果たされた場合があります。 ネットワーク デバイスの受信バッファーが使い果たされた場合、ネットワーク デバイスは断片化されたパケットの再構築を試みますが、パケットを格納して再び取り出すためのリソースがありません。

断片化は否定的な操作と認識されることがありますが、断片化のサポートは、インターネット経由で多様なネットワークに接続するために必要です。

#### <a name="benefits-and-consequences-of-modifying-the-mtu"></a>MTU の変更の利点と結果

一般に、MTU を大きくすると、より効率的なネットワークを作成できます。 送信されるすべてのパケットには、元のパケットに追加される追加のヘッダー情報があります。 パケットの増加はヘッダーのオーバーヘッドが大きくなることを意味し、結果としてネットワークの効率が低下します。

たとえば、イーサネット ヘッダーのサイズは、14 バイトに、フレームの一貫性を確保するための 4 バイトのフレーム チェック シーケンス (FCS) が加算されます。 2,000 バイトのパケットが 1 つ送信された場合、18 バイトのイーサネット オーバーヘッドがネットワークに追加されます。 パケットが 1,500 バイトのパケットと 500 バイトのパケットに断片化されている場合は、各パケットが 18 バイトのイーサネット ヘッダーを持ち、合計で 36 バイトになります。 一方、単一の 2,000 バイトのパケットは 18 バイトのイーサネット ヘッダーだけを持ちます。

MTU 自体を増やしても、より効率的なネットワークが作成されるとは限らないことに注意することが重要です。 アプリケーションで 500 バイトのパケットのみを送信する場合、MTU が 1,500 バイトか 9,000 バイトかにかかわらず、同じヘッダー オーバーヘッドが存在します。 ネットワークの効率を上げるには、MTU との相対で、より大きなパケット サイズを使用する必要もあります。

#### <a name="azure-and-vm-mtu"></a>Azure と VM の MTU

Azure VM の既定の MTU は 1,500 バイトです。 Azure Virtual Network スタックでは、1,400 バイトでのパケットの断片化が試行されます。 ただし、Azure Virtual Network スタックでは、IP ヘッダーに "断片化禁止" ビットが設定されている場合に、2,006 バイトまでのパケットが許可されます。

パケットは 1,400 バイトで断片化されますが、VM の MTU は 1,500 であるため、この断片化は Azure Virtual Network スタックが本質的に非効率であることを意味していないことに注意することが重要です。 実際には、ネットワーク パケットの大部分が 1,400 バイトまたは 1,500 バイトをはるかに下回っています。

#### <a name="azure-and-fragmentation"></a>Azure と断片化

現在、Azure の Virtual Network スタックは、"順序が正しくないフラグメント"、つまり元の断片化された順序で到着しない断片化されたパケットを削除するように構成されています。 これらのパケットは主に、2018 年 11 月に発表された FragmentStack と呼ばれるネットワーク セキュリティの脆弱性が原因で削除されます。

FragmentSmack は、断片化された IPv4 および IPv6 パケットの再構築が Linux カーネルによって処理される方法の欠陥です。 リモートの攻撃者は、この欠陥を利用して、コストの高いフラグメント再構築操作をトリガーします。これは、ターゲット システム上での CPU 使用の増加やサービス拒否攻撃につながります。

#### <a name="tune-the-mtu"></a>MTU のチューニング

Azure VM では、他のオペレーティング システムと同様に、構成可能な MTU がサポートされます。 ただし、MTU を構成する際には、Azure 内で発生する、上記で詳しく説明した断片化を考慮する必要があります。

Azure では、VM の MTU を増やすことをお客様にお勧めしていません。 この説明は、現在 Azure がどのように MTU を実装し、断片化を実行しているかを詳しく説明することが目的です。

> [!IMPORTANT]
>MTU を大きくしてもパフォーマンスの向上は見られず、アプリケーションのパフォーマンスに悪影響を及ぼす可能性があります。
>
>

#### <a name="large-send-offload-lso"></a>Large Send Offload (LSO)

Large Send Offload (LSO) では、パケットのセグメント化をイーサネット アダプターにオフロードすることで、ネットワーク パフォーマンスを向上させることができます。 LSO が有効になっている場合、TCP/IP スタックでは大きな TCP パケットが作成され、転送前にセグメント化のためにイーサネット アダプターに送信されます。 LSO の利点は、MTU に準拠したパケット サイズへのパケットのセグメント化から CPU を解放でき、ハードウェアで実行されているイーサネット インターフェイスにその処理をオフロードできることです。 LSO の利点について詳しくは、[Microsoft ネットワーク アダプターのパフォーマンスに関するドキュメント](https://docs.microsoft.com/windows-hardware/drivers/network/performance-in-network-adapters#supporting-large-send-offload-lso)をご覧ください。

LSO が有効になっている場合、Azure のお客様では、パケット キャプチャの実行時に大きなフレーム サイズが見られることがあります。 これらの大きなフレーム サイズにより、一部のお客様は、その断片化または巨大な MTU が使用されていないのに使用されていると考える可能性があります。 LSO により、イーサネット アダプターは、より大きな TCP パケットを作成するために、より大きな MSS を TCP/IP スタックにアドバタイズできます。 この非セグメント化フレーム全体がイーサネット アダプターに転送され、VM 上で実行されるパケット キャプチャに表示されます。 ただし、パケットは、イーサネット アダプターによってイーサネット アダプターの MTU に従って多くの小さなフレームに分割されます。

### <a name="tcpmss-window-scaling-and-pmtud"></a>TCP/MSS ウィンドウ スケーリングと PMTUD

#### <a name="explanation-of-tcp-mss"></a>TCP MSS の説明

TCP 最大セグメント サイズ (MSS) は、TCP セグメントの最大サイズを設定して TCP パケットの断片化を避けるための設定です。 オペレーティング システムでは通常、MSS が MSS = MTU - IP & TCP ヘッダー サイズ (それぞれ 20 バイトまたは合計 40 バイト) として設定されます。 したがって、1,500 の MTU とのインターフェイスでは MSS が 1,460 になります。 ただし、MSS は構成可能です。

この設定は、TCP セッションがソースと宛先の間に設定されている場合に、TCP 3 ウェイ ハンドシェイクで合意されます。 両方の側が MSS 値を送信し、2 つのうち小さい方が TCP 接続に使用されます。

Azure VPN Gateway をはじめとする VPN ゲートウェイのような中間ネットワーク デバイスには、最適なネットワーク パフォーマンスを実現するためにソースと宛先に依存せずに MTU を調整する機能があります。 そのため、ソースと宛先単独の MTU は、実際の MSS 値の唯一の要素ではないことにご注意ください。

#### <a name="explanation-of-path-mtu-discovery-pmtud"></a>パス MTU 検出 (PMTUD) の説明

MSS がネゴシエートされても、それは使用できる実際の MSS を示していない可能性があります。ソースと宛先間のパス上にある他のネットワーク デバイスの MTU 値がソースと宛先の MTU 値よりも小さい場合があるからです。 この場合、パケットよりも小さい MTU を持つデバイスでは、パケットが削除され、その MTU を格納した "Internet Control Message Protocol (ICMP) Fragmentation Needed (Type 3, Code 4)" (インターネット制御メッセージ プロトコル (ICMP) 断片化が必要 (タイプ 3、コード 4)) メッセージが返信されます。 この ICMP メッセージにより、ソース ホストはそのパス MTU を適宜縮小できます。 そのプロセスは、パス MTU 検出と呼ばれます。

PMTUD のプロセスは、本質的に非効率的で、ネットワークのパフォーマンスに影響を及ぼします。 ネットワーク パスの MTU を超えるパケットが送信された場合、それらのパケットはより小さい MSS で再送信される必要があります。 おそらくパス上のネットワーク ファイアウォール (一般に PMTUD ブラックホールと呼ばれます) が原因で送信者が ICMP Fragmentation Needed (ICMP 断片化が必要) パケットを受信しない場合、送信者は MSS を縮小する必要があることを認識せず、パケットを再送信し続けます。 このため、Azure VM の MTU を増やすことはお勧めしません。

#### <a name="vpn-considerations-with-mtu"></a>MTU での VPN の考慮事項

カプセル化 (IPSec VPN など) を実行する VM を使用しているお客様には、パケット サイズと MTU に追加の影響が及ぶ場合があります。 VPN によって追加されるヘッダーが元のパケットに追加されるので、パケット サイズが増加し、より小さい MSS が必要になります。

Azure に関する現在の推奨事項は、TCP MSS クランプを 1,350 バイトに設定し、トンネル インターフェイス MTU を 1,400 に設定することです。 詳しくは、[VPN デバイスと IPSec/IKE パラメーターに関するページ](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpn-devices)をご覧ください。

### <a name="latency-round-trip-time-and-tcp-window-scaling"></a>待ち時間、ラウンドトリップ時間、TCP ウィンドウ スケーリング

#### <a name="latency-and-round-trip-time"></a>待ち時間とラウンドトリップ時間

ネットワーク待ち時間は、光ファイバー ネットワーク上では光の速度に左右されます。 実際には、2 台のネットワーク デバイス間のラウンドトリップ時間 (RTT) により、TCP のネットワーク スループットも事実上支配されます (実際的な最大値)。

| | | | |
|-|-|-|-|
|ルート|Distance|一方向の時間|ラウンドトリップ時間 (RTT)|
|ニューヨークからサンフランシスコへ|4,148 km|21 ミリ秒|42 ミリ秒|
|ニューヨークからロンドンへ|5,585 km|28 ミリ秒|56 ミリ秒|
|ニューヨークからシドニーへ|15,993 km|80 ミリ秒|160 ミリ秒|

この表は、2 つの場所の間の直線距離を示していますが、ネットワークでは通常、距離は直線距離より長くなります。 光の速度によって左右される最小 RTT を計算する単純な数式は、最小 RTT = 2 * (km 単位の距離 / 伝達速度) です。

伝達速度には標準値の 200 を使用できます。この値は、1 ミリ秒間に光が移動するメートル単位の距離です。

ニューヨークからサンフランシスコへの例では、これは直線距離で 4,148 km です。 最小 RTT = 2 * (4,148 / 20)。 式の出力は、ミリ秒単位になります。

実際には 2 つの場所間の物理的な距離は固定なので、最大のネットワーク パフォーマンスが必要な場合、最も論理的なオプションは、それらの間の距離が最小となる宛先を選択することです。 次に、仮想ネットワーク内で、トラフィックのパスが最適化され、待ち時間が短縮されるような設計上の決定を行うことができます。 これらの仮想ネットワークの考慮事項については、以下の「ネットワーク設計に関する考慮事項」のセクションで説明します。

#### <a name="latency-and-round-trip-time-effects-on-tcp"></a>TCP に対する待ち時間とラウンドトリップ時間の影響

ラウンド トリップ時間 (RTT) は、TCP の最大スループットに直接影響します。 TCP プロトコルには、ウィンドウ サイズの概念があります。 ウィンドウ サイズは、送信者が受信者から受信確認を受信する前に TCP 接続経由で送信できるトラフィックの最大量です。 TCP MSS が 1,460 に設定され、TCP ウィンドウ サイズが 65,535 に設定されている場合、送信者は受信者から受信確認を受信する前に 45 パケットを送信できます。 受信確認を受信していない場合、送信者は再送信します。 この例では、TCP ウィンドウ サイズ / TCP MSS = 送信されるパケット数です。 または、65,535 / 1,460 は 45 に丸められます。

信頼性の高いデータ配信を創出するメカニズムとしてのこの "受信確認待機中" 状態は、RTT が TCP のスループットに影響を与える事実上の原因です。 送信者が受信確認を待機する期間が長くなるほど、他のデータを送信する前に待機する必要のある時間も長くなります。

1 つの TCP 接続の最大スループットを計算する数式は次のとおりです。ウィンドウ サイズ / (RTT 待ち時間 (ミリ秒) / 1000) = 最大バイト数/秒。 次の表は、読みやすくするために MB 単位で書式設定されており、1 つの TCP 接続の最大 MB/秒でのスループットを示しています。

| | | | |
|-|-|-|-|
|TCP ウィンドウ サイズ (バイト単位)|RTT 待ち時間<br/>(ミリ秒単位)|最大値<br/>MB/秒でのスループット|最大値<br/> Mb/秒でのスループット|
|65,535|1|65.54|524.29|
|65,535|30|2.18|17.48|
|65,535|60|1.09|8.74|
|65,535|90|.73|5.83|
|65,535|120|.55|4.37|

パケット損失がある場合、送信者が既に送信されたデータを再送信するときに、TCP 接続の最大スループットが低下します。

#### <a name="explanation-of-tcp-window-scaling"></a>TCP ウィンドウ スケーリングの説明

TCP ウィンドウ スケーリングは、TCP ウィンドウ サイズを動的に増やして、受信確認が必要になる前に送信できるデータ量を増やすという概念です。 前の例では、受信確認が必要になる前に 45 パケットが送信されます。 受信確認の前に送信されるパケット数が増やされた場合、送信者が受信確認を待機する回数が減るので TCP の最大スループットも向上します。

TCP のスループットについて次の単純な表で説明します。

| | | | |
|-|-|-|-|
|TCP ウィンドウ サイズ<br/>(バイト単位)|RTT 待ち時間 (ミリ秒単位)|最大値<br/>MB/秒でのスループット|最大値<br/> Mb/秒でのスループット|
|65,535|30|2.18|17.48|
|131,070|30|4.37|34.95|
|262,140|30|8.74|69.91|
|524,280|30|17.48|139.81|

ただし、TCP ウィンドウ サイズの TCP ヘッダー値は 2 バイトの長さなので、受信ウィンドウの最大値は 65,535 になります。 最大ウィンドウ サイズを大きくするために、TCP ウィンドウ スケール ファクターが導入されました。

スケール ファクターも、オペレーティング システムで構成できる設定です。 スケール ファクターを使用して TCP ウィンドウ サイズを計算する数式は次のとおりです。TCP ウィンドウ サイズ= TCP ウィンドウ サイズ (バイト単位) \* (2^ スケール ファクター)。 ウィンドウ スケール ファクターが 3 でウィンドウ サイズが 65,535 の場合、計算は次のようになります。65,535 \* (2^3) = 262,140 バイト。 14 のスケール ファクターでは TCP ウィンドウ サイズが 14 (最大オフセットを許可) になり、TCP ウィンドウ サイズは 1,073,725,440 バイト (8.5 ギガビット) になります。

#### <a name="support-for-tcp-window-scaling"></a>TCP ウィンドウ スケーリングのサポート

Windows には、接続の種類ごとに異なるスケール ファクターを設定する機能があります。接続には複数のクラス (データセンター、インターネットなど) があります。 Get-NetTCPConnection powershell コマンドを使用して、ウィンドウ スケーリング接続の分類を確認できます。

```powershell
Get-NetTCPConnection
```

Get-NetTCPSetting powershell コマンドを使用して、各クラスの値を確認できます。

```powershell
Get-NetTCPSetting
```

初期の TCP ウィンドウ サイズと TCP スケール ファクターは、Windows 上で Set-NetTCPSetting powershell コマンドを使用して設定できます。 詳しくは、「[Set-NetTCPSetting](https://docs.microsoft.com/powershell/module/nettcpip/set-nettcpsetting?view=win10-ps)」のページをご覧ください

```powershell
Set-NetTCPSetting
```

AutoTuningLevel に対する効果的な TCP 設定は次のとおりです。

| | | | |
|-|-|-|-|
|AutoTuningLevel|スケール ファクター|スケール乗数|数式<br/>(最大ウィンドウ サイズを計算)|
|Disabled|なし|なし|ウィンドウ サイズ|
|制限付き|4|2^4|ウィンドウ サイズ * (2^4)|
|厳しく制限|2|2^2|ウィンドウ サイズ * (2^2)|
|Normal|8|2^8|ウィンドウ サイズ (2^8)|
|試験段階|14|2^14|ウィンドウ サイズ * (2^14)|

これらの設定は TCP のパフォーマンスに影響する可能性が最も高いものですが、Azure で制御されないインターネット上の他の多くの要素も TCP のパフォーマンスに影響する可能性があることに注意する必要があります。

#### <a name="increase-mtu-size"></a>MTU サイズを増やす

たずねるべき論理的な質問は、「MTU が大きいほど MSS が大きくなるので、MTU を増やすと TCP のパフォーマンスが向上しますか?」です。 単純な回答は、おそらく「いいえ」です。 説明したように、パケット サイズには、TCP トラフィック以外にも適用される長所と短所があります。 前述のように、TCP のスループット パフォーマンスに影響を与える最も重要な要素は、TCP ウィンドウ サイズ、パケット損失、RTT です。

> [!IMPORTANT]
> Azure では、Azure のお客様に、仮想マシン上の既定の MTU 値を変更することをお勧めしていません。
>
>

### <a name="accelerated-networking-and-receive-side-scaling"></a>高速ネットワークと受信側のスケーリング

#### <a name="accelerated-networking"></a>高速ネットワーク

仮想マシンのネットワーク機能は、これまで VM ゲストとハイパーバイザー/ホストの両方で CPU を集中的に使用してきました。 ホストを通過するすべてのパケットは、ホストの CPU によってソフトウェア内で処理されます。これには、すべての仮想ネットワークのカプセル化/カプセル化解除が含まれます。 そのため、ホストを通過するトラフィックが増えると、CPU の負荷が高くなります。 また、ホストの CPU が他の操作を実行していてビジー状態の場合は、ネットワークのスループットと待ち時間にも影響します。 この問題は、高速ネットワークによって対処されてきました。

高速ネットワークは、Azure のプログラミング可能な社内ハードウェアと SR-IOV などのテクノロジを使用して、一貫性のある非常に短いネットワーク待ち時間を実現します。 CPU 内に存在する Azure のソフトウェア定義ネットワーク スタックのほとんどを FPGA ベースの SmartNIC に移行することで、エンドユーザー アプリケーションによるコンピューティング サイクルの再生が可能になり、VM の負荷軽減、ジッターの低減、一貫性のある待ち時間が実現されます。 つまり、パフォーマンスがより決定論的になります。

高速ネットワークは、ゲスト VM がホストをバイパスしてホストの SmartNIC との直接データパスを確立できるようにすることで、パフォーマンス向上を実現します。 高速ネットワークの利点は次のとおりです。

- **待ち時間の短縮/1 秒あたりのパケット数 (pps) の向上**:データパスから仮想スイッチを削除することで、ホストにおけるパケットのポリシー処理に必要な時間がなくなるため、VM 内で処理できるパケット数が増加します。

- **ジッターの削減**:仮想スイッチの処理は、適用するポリシーの量と、処理を行う CPU のワークロードによって異なります。 ハードウェアへのポリシーの適用をオフロードすると、パケットが直接 VM に配信され、ホストと VM 間の通信とソフトウェアによる干渉やコンテキスト スイッチがなくなるため、そのばらつきはなくなります。

- **CPU 使用率の削減**:ホストの仮想スイッチをバイパスすることによって、ネットワーク トラフィックを処理するための CPU 使用率を削減できます。

高速ネットワークは VM ごとに明示的に有効にする必要があります。 VM 上で高速ネットワークを有効にする手順については、「[高速ネットワークを使った Linux 仮想マシンの作成](https://docs.microsoft.com/azure/virtual-network/create-vm-accelerated-networking-cli)」のページをご覧ください。

#### <a name="receive-side-scaling-rss"></a>Receive Side Scaling (RSS)

Receive Side Scaling は、受信処理をマルチプロセッサ システム上の複数の CPU に分散することで、ネットワーク トラフィックの受信をより効率的に分散するネットワーク ドライバー テクノロジです。 簡単に言うと、RSS では、1 つだけではなくすべての使用可能な CPU が使用されるので、システムで処理できる受信トラフィックが増えます。 RSS の技術的な説明については、「[Receive Side Scaling の概要](https://docs.microsoft.com/windows-hardware/drivers/network/introduction-to-receive-side-scaling)」のページをご覧ください。

RSS は、VM 上で高速ネットワークが有効になっているときに、最大のパフォーマンスを実現するために必要です。 高速ネットワークが有効になっていない VM 上でも、RSS の使用にメリットがあります。 RSS が有効になっているかどうかを判断する方法と、これを有効にするための構成の概要については、「[Azure 仮想マシンのネットワーク スループットの最適化](http://aka.ms/FastVM)」のページをご覧ください。

### <a name="tcp-time-wait-and-time-wait-assassination"></a>TCP 待機中と待機中アセシネーション

ネットワークとアプリケーションのパフォーマンスに影響を与えるもう 1 つの一般的な問題は、TCP 待機中設定です。 多くのソケットを開閉しているビジーな VM 上では、クライアントまたはサーバー (ソース IP:送信元ポート + 宛先 IP:送信先ポート) として、TCP の通常の動作中に、特定のソケットが最終的に長時間待機中状態になることがあります。 この "待機中" 状態は、ソケットを閉じる前に、そのソケットで他のデータを配信できることを意味します。 そのため、TCP/IP スタックは一般に、クライアントの TCP SYN パケットを自動的に削除することにより、ソケットの再利用を防止します。

このソケットが待機中状態となる時間の長さは構成可能ですが、30 秒から 240 秒の範囲にすることができます。 ソケットは有限のリソースであり、同時に使用できるソケット数は構成可能です (一般に、潜在的なソケット数は約 30,000 です)。 この数が使い果たされるか、クライアントとサーバーの待機中設定が一致しない場合に、VM が待機中状態のソケットを再利用しようとすると、TCP SYN パケットが自動的に削除されているため新しい接続は失敗します。

通常、送信ソケットのポート範囲の値に加えて、TCP 待機中設定とソケットの再利用は、オペレーティング システムの TCP/IP スタック内で構成可能です。 これらの数値を変更すると、スケーラビリティが向上する可能性がありますが、シナリオによっては、相互運用性の問題が発生する可能性があるので、慎重に変更する必要があります。

このスケーリング制限に対処するために、待機中アセシネーションという機能が導入されました。 待機中アセシネーションにより、新しい接続の IP パケット内のシーケンス番号が以前の接続の最後のパケットのシーケンス番号を超えたときのような一部のシナリオで、ソケットを再利用できます。 この場合、オペレーティング システムにより、新しい接続の確立 (新しい SYN ACK の受け入れ) が許可され、待機中状態だった以前の接続が強制的に終了されます。 この機能は Azure 内の Windows VM 上で現在サポートされており、他の VM 内でのサポートについては、Azure のお客様とそれぞれの OS ベンダーが調べる必要があります。

TCP 待機中設定と送信元ポート範囲の構成に関するドキュメントについては、「[Settings that can be Modified to Improve Network Performance (ネットワーク パフォーマンスを向上させるために変更可能な設定)](https://docs.microsoft.com/biztalk/technical-guides/settings-that-can-be-modified-to-improve-network-performance)」をご覧ください。

## <a name="virtual-network-factors-that-can-affect-performance"></a>パフォーマンスに影響する可能性のある仮想ネットワーク要素

### <a name="vm-maximum-outbound-throughput"></a>VM の最大送信スループット

Azure の VM には多様なサイズと種類があり、パフォーマンス機能の組み合わせもそれぞれ異なっています。 このようなパフォーマンス機能の 1 つがネットワーク スループット (帯域幅) で、メガビット/秒 (Mbps) で測定されます。 仮想マシンは共有ハードウェアでホストされているため、同じハードウェアを共有する仮想マシン間でネットワーク容量が公平に分配される必要があります。 大きな仮想マシンには、小さい仮想マシンよりも相対的に多くの帯域幅が割り当てられます。

各仮想マシンに割り当てられたネットワーク帯域幅は、仮想マシンからのエグレス (送信) トラフィックで測定されます。 仮想マシンから送信されるすべてのネットワーク トラフィックは、送信先に関係なく、割り当てられた制限に達するまでカウントされます。 たとえば、ある仮想マシンの制限が 1,000 Mbps である場合、送信トラフィックの送信先が同じ仮想ネットワーク内の仮想マシンであっても Azure 外部であっても、この制限が適用されます。
受信は、直接的には測定も制限もされません。 ただし、CPU やストレージの制限などの他の要素があり、仮想マシンの受信データ処理機能に影響する可能性はあります。

高速ネットワークは、ネットワーク パフォーマンス (待ち時間、スループット、CPU 使用率など) の向上を目的として設計された機能です。 高速ネットワークは仮想マシンのスループットを向上させますが、向上できるのは仮想マシンに割り当てられた帯域幅までです。

Azure の仮想マシンには、少なくとも 1 つ (複数可) のネットワーク インターフェイスが接続されている必要があります。 仮想マシンに割り当てられる帯域幅は、仮想マシンに接続されているすべてのネットワーク インターフェイス全体の送信トラフィックの合計です。 つまり、帯域幅は仮想マシンごとに割り当てられており、仮想マシンに接続されているネットワーク インターフェイスの数は関係ありません。
 
VM の各サイズで想定される送信スループットとサポートされるネットワーク インターフェイスの数について、ここで詳しく説明します。 最大スループットを確認するには、型 (汎用など) を選択し、結果として表示されるページでサイズ シリーズ (Dv2 シリーズなど) を選択します。 各シリーズの表の最後に、最大 NIC 数/想定ネットワーク パフォーマンス (Mbps) というネットワーク仕様の列があります。

このスループット制限が仮想マシンに適用されます。 スループットは、次の要因には影響されません。

- **ネットワーク インターフェイスの数**:帯域幅の制限は、仮想マシンからのすべての送信トラフィックの累積です。

- **高速ネットワーク**:この機能は公開された制限まで達成するためには役立ちますが、制限自体は変更されません。

- **トラフィックの送信先**:すべての送信先が、送信制限に達するまでカウントされます。

- **プロトコル**:すべてのプロトコルに対するすべての送信トラフィックが、制限に達するまでカウントされます。

[VM の種類ごとの最大帯域幅の表を参照するには、このページにアクセスし](https://docs.microsoft.com/azure/virtual-machines/windows/sizes)、それぞれの VM の種類をクリックします。 それぞれの種類のページで、表は最大 NIC と予想される最大のネットワーク帯域幅を示します。

VM ネットワーク帯域幅について詳しくは、「[仮想マシンのネットワーク帯域幅](http://aka.ms/AzureBandwidth)」をご覧ください。

### <a name="internet-performance-considerations"></a>インターネットのパフォーマンスに関する考慮事項

この記事全体を通して説明するように、インターネット上の要素と Azure の制御外の要素がネットワーク パフォーマンスに影響する可能性があります。 これらの要素は次のとおりです。

- **待ち時間**:2 つの宛先の間のラウンドトリップ時間は、中間ネットワーク上の問題、可能な "最短" 距離のパスをとることができないトラフィック、および最適ではないピアリング パスの影響を受ける可能性があります

- **パケット損失**:パケット損失は、ネットワークの輻輳、物理パスの問題、およびパフォーマンスが平均を下回るネットワーク デバイスが原因となっている可能性があります

- **MTU サイズ/断片化**:パス上での断片化は、データの到着遅延や正しくない順序でのパケットの到着につながる場合があり、これがパケットの配信に影響する可能性があります

Traceroute は、ソース デバイスと宛先デバイス間のすべてのネットワーク パス上でネットワーク パフォーマンスの特性 (パケット損失や待ち時間など) を測定するための優れたツールです。

### <a name="network-design-considerations"></a>ネットワーク設計に関する考慮事項

上記の考慮事項と共に、仮想ネットワークのトポロジが仮想ネットワークのパフォーマンスに影響することがあります。 たとえば、トラフィックを単一ハブの仮想ネットワークにグローバルにバックホールするハブ アンド スポーク設計では、ネットワーク遅延が生じるので、ネットワークの全体的なパフォーマンスに影響があります。 同様に、ネットワーク トラフィックが通過するネットワーク デバイスの数も全体的な待ち時間に影響することがあります。 たとえば、ハブ アンド スポーク設計では、トラフィックがインターネットに送信される前にスポークのネットワーク仮想アプライアンスとハブの仮想アプライアンスを通過する場合、ネットワーク仮想アプライアンスによって待ち時間が生じる可能性があります。

### <a name="azure-regions-virtual-networks-and-latency"></a>Azure リージョン、仮想ネットワーク、待ち時間

Azure リージョンは、一般的な地理的領域内に存在する複数のデータ センターで構成されます。 これらのデータセンターは、物理的に隣り合っていない可能性があり、場合によっては 10 km 単位で離れていることがあります。 仮想ネットワークは Azure の物理データ センター ネットワークの上にある論理的なオーバーレイであり、仮想ネットワークはデータ センター内の任意の特定のネットワーク トポロジを意味しません。 たとえば、VM A と VM B が同じ仮想ネットワークとサブネット内にあっても、異なるラック、列、さらにはデータセンターにある可能性があります。 これらを分離している光ファイバー ケーブルの長さは、1 フィートの場合も数 km の場合もあります。 この現実は、異なる VM 間に可変の待ち時間 (数ミリ秒の差異) をもたらす可能性があります。

この地理的な配置、したがって 2 つの VM 間の待ち時間は、可用性セットと可用性ゾーンの構成の影響を受ける可能性がありますが、リージョン内のデータセンター間の距離はリージョンに固有で、主にリージョン内のデータセンター トポロジの影響を受けます。

### <a name="source-nat-port-exhaustion"></a>送信元 NAT ポートの不足

Azure 内のデプロイでは、パブリック インターネットまたはパブリック IP 空間、あるいはその両方にある、Azure 外部のエンドポイントと通信できます。 インスタンスがこの送信接続を開始すると、Azure によってプライベート IP アドレスがパブリック IP アドレスに動的にマッピングされます。 このマッピングが作成されると、この送信フローの戻りトラフィックも、フローの送信元であるプライベート IP アドレスに到達できます。

送信接続ごとに、このマッピングが Azure Load Balancer によって一定期間維持される必要があります。 Azure のマルチテナントの性質により、すべての VM のすべての送信フローについてこのマッピングを保持すると、大量のリソースが消費される可能性があります。 そのため、Azure Virtual Network の構成に基づいて制限が設定されます。 または、より正確に言うと、Azure VM では特定の時点に特定の数の送信接続のみ確立できます。 これらの制限に達すると、Azure VM ではそれ以上送信接続を確立できません。

ただし、この動作は構成可能です。 [SNAT と SNAT ポートの不足] について詳しくは、[こちらの記事](https://docs.microsoft.com/azure/load-balancer/load-balancer-outbound-connections)をご覧ください。

## <a name="measure-network-performance-on-azure"></a>Azure 上でのネットワーク パフォーマンスの測定

この記事のパフォーマンスの最大値は、2 つの VM 間のネットワーク待ち時間/ラウンドトリップ時間 (RTT) に関連しています。 ここでは、待ち時間/RTT だけでなく TCP のパフォーマンスと VM ネットワークのパフォーマンスもテストする方法についての推奨事項を示します。 前に説明した TCP/IP とネットワークの値は、以下で説明する手法を使用して調整し、パフォーマンスをテストできます。 待ち時間、MTU、MSS、およびウィンドウ サイズの値は上に示した計算で使用でき、理論上の最大値をテスト中に観察された実際の値と比較できます。

### <a name="measure-round-trip-time-and-packet-loss"></a>ラウンドトリップ時間とパケット損失の測定

TCP のパフォーマンスは、RTT とパケット損失に大きく依存します。 RTT とパケット損失を測定する最も簡単な方法は、Windows と Linux で使用可能な ping ユーティリティを使用することです。 ping の出力は、ソースと宛先の間の最小/最大/平均待ち時間だけでなく、パケット損失も示します。 ping では、既定で ICMP プロトコルが使用されます。 TCP RTT をテストするには、PsPing を使用できます。 PsPing について詳しくは、[こちらのリンク](https://docs.microsoft.com/sysinternals/downloads/psping)をご覧ください。

### <a name="measure-actual-throughput-of-a-tcp-connection"></a>TCP 接続の実際のスループットの測定

NTttcp は、Linux または Windows VM の TCP パフォーマンスをテストするために使用するツールです。 NTttcp を使用して、さまざまな TCP 設定を調整し、メリットをテストできます。 NTttcp について詳しくは、以下のリンクをご覧ください。

- [帯域幅/スループットのテスト (NTttcp)](https://aka.ms/TestNetworkThroughput)

- [NTttcp ユーティリティ](https://gallery.technet.microsoft.com/NTttcp-Version-528-Now-f8b12769)

### <a name="measure-actual-bandwidth-of-a-virtual-machine"></a>仮想マシンの実際の帯域幅の測定

異なる VM の種類や高速ネットワークなどのパフォーマンスのテストは、Linux と Windows 上でも使用可能な Iperf というツールを使用してテストできます。 Iperf では、TCP または UDP を使用してネットワークの全体的なスループットをテストできます。 Iperf を使用した TCP のスループット テストは、この記事で説明した要素 (待ち時間や RTT など) の影響を受けます。 そのため、単純な最大スループットのテストでは UDP がより優れた結果を生成する可能性があります。

追加情報は以下にあります。

- [Expressroute ネットワーク パフォーマンスのトラブルシューティング](https://docs.microsoft.com/azure/expressroute/expressroute-troubleshooting-network-performance)

- [仮想ネットワークへの VPN スループットを検証する方法](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-validate-throughput-to-vnet)

### <a name="detect-inefficient-tcp-behaviors"></a>非効率的な TCP 動作の検出

Azure のお客様では、ネットワーク パフォーマンスの問題を示している可能性のある TCP フラグ (SACK、DUP ACK、RETRANSMIT、FAST RETRANSMIT) の付いた TCP パケットがパケット キャプチャ内に見られる場合があります。 具体的には、これらのパケットは、パケット損失の結果としてのネットワークの非効率性を示しています。 ただし、パケット損失は、必ずしも Azure パフォーマンスの問題が原因ではありません。 パフォーマンスの問題は、アプリケーション、オペレーティング システム、またはその他の Azure プラットフォームに直接には関連しない問題の結果である可能性があります。 ネットワーク上の多少の再送信や重複する ACK は正常であり、TCP プロトコルは信頼できるものとして構築されていることに注意することも重要です。 また、パケット キャプチャにおけるこれらの TCP パケットの証拠は、過剰でない限り、必ずしもシステム的なネットワークの問題を示していません。

ただし、これらのパケットの種類は、TCP のスループットが他のセクションで説明した理由により最大パフォーマンスを達成していないことの指標であることを明確に示す必要があります。
