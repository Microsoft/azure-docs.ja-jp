---
title: Personalizer のしくみ - Personalizer
titleSuffix: Azure Cognitive Services
description: Personalizer は、機械学習を使用して、コンテキスト内で使用するアクションを発見します。 各学習ループには、Rank 呼び出しと Reward 呼び出しを介して送信したデータについてのみトレーニングされたモデルがあります。 すべての学習ループは、互いに完全に独立しています。
author: diberry
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 09/13/2019
ms.author: diberry
ms.openlocfilehash: 7c163dacae24749dbe309bca33bac016a3be7aa5
ms.sourcegitcommit: e97a0b4ffcb529691942fc75e7de919bc02b06ff
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 09/15/2019
ms.locfileid: "71002886"
---
# <a name="how-personalizer-works"></a>Personalizer のしくみ

Personalizer は、機械学習を使用して、コンテキスト内で使用するアクションを発見します。 各学習ループには、**Rank** 呼び出しと **Reward** 呼び出しを介して送信したデータについてのみトレーニングされたモデルがあります。 すべての学習ループは、互いに完全に独立しています。 パーソナル化したいアプリケーションの各部分または動作の学習ループを作成します。

ループごとに、現在のコンテキストに基づいて次のものと一緒に **Rank API を呼び出します**。

* 使用できるアクションのリスト: 最上位のアクションを選択するコンテンツ項目。
* [コンテキストの特徴](concepts-features.md)のリスト: ユーザー、コンテンツ、コンテキストなどのコンテキストに関連するデータ。

**Rank** API では、次のいずれかを使用することを決定できます。

* "_活用_": 過去のデータに基づいて最善のアクションを決定するための現在のモデル。
* "_探索_": 最上位のアクションではなく、別のアクションを選択します。

**Reward** API:

* 各 Rank 呼び出しの特徴と報酬スコアを記録することによってモデルをトレーニングするためのデータを収集します。
* そのデータを使用して、"_学習ポリシー_" に指定された設定に基づいてモデルを更新します。

## <a name="architecture"></a>アーキテクチャ

次の図は、Rank と Reward の呼び出しを呼び出すアーキテクチャの流れを示しています。

![代替テキスト](./media/how-personalizer-works/personalization-how-it-works.png "パーソナル化のしくみ")

1. Personalizer は、内部 AI モデルを使用して、アクションの順位を決定します。
1. このサービスは、現在のモデルを活用するか、モデルの新しい選択肢を探索するかを決定します。  
1. 順位付けの結果は EventHub に送信されます。
1. Personalizer が報酬を受け取ると、その報酬が EventHub に送信されます。 
1. 順位と報酬が関連付けられます。
1. AI モデルが、相関関係の結果に基づいて更新されます。
1. 推論エンジンが、新しいモデルで更新されます。 

## <a name="research-behind-personalizer"></a>Personalizer の背後にある研究

Personalizer は、論文、研究活動、および現在進行中の Microsoft Research の探索分野を含む、[強化学習](concepts-reinforcement-learning.md)分野の最先端の科学および研究に基づいています。

## <a name="terminology"></a>用語集

* **学習ループ**: 学習ループは、パーソナル化によるメリットが得られるアプリケーションのあらゆる部分に対して作成できます。 パーソナル化するエクスペリエンスが複数ある場合は、それぞれにループを作成します。 

* **Actions**:アクションは、商品やプロモーションなど、選択対象のコンテンツ項目です。 Personalizer は、Rank API を介して、"_報酬アクション_" と呼ばれる、ユーザーに表示する最上位のアクションを選択します。 各アクションは、順位付けの要求と共に送信される特徴を備えることができます。

* **コンテキスト**:より正確な順位を提供するために、コンテキストに関する情報を提供します。次に例を示します。
    * ユーザー。
    * ユーザーが使用しているデバイス。 
    * 現在の時刻。
    * 現在の状況に関するその他のデータ。
    * ユーザーまたはコンテキストに関する履歴データ。

    特定のアプリケーションにおいて異なるコンテキスト情報がある場合があります。 

* **[特徴](concepts-features.md)** : コンテンツ項目またはユーザー コンテキストに関する情報のユニット。

* **報酬**: Rank API に応答してユーザーから返されたアクションの尺度 (0 から 1 のスコア)。 0 から 1 の値は、その選択がパーソナル化のビジネス目標の達成にどのように役立ったかに基づいて、ビジネス ロジックによって設定されます。 

* **探索**: Personalizer サービスは、最善のアクションを返す代わりに、ユーザーに対して別のアクションを選択するときに探索を行っています。 Personalizer サービスは、ドリフトや停滞を回避し、探索することで進行中のユーザーの動作に適応できます。 

* **実験期間**: そのイベントに対して Rank 呼び出しが行われた時点からの、Personalizer サービスが報酬を待つ時間の長さ。

* **非アクティブなイベント**: 非アクティブなイベントとは、Rank が呼び出されたときにクライアント アプリケーションによる決定によりユーザーに結果が表示されるかどうかが不明なイベントを表します。 非アクティブなイベントを使用すると、パーソナル化の結果を作成して保存した後、機械学習モデルに影響を与えることなくそれらを破棄することを決定できます。

* **モデル**:Personalizer モデルは、ユーザーの動作に関して学習したすべてのデータを取得し、Rank と Reward の呼び出しに送信した引数と学習ポリシーで決定されたトレーニング動作の組み合わせからトレーニング データを取得します。 

* **学習ポリシー**:Personalizer によるすべてのイベントに対するモデルのトレーニング方法は、機械学習アルゴリズムの動作方法に影響するいくつかのメタパラメーターによって決まります。 新しい Personalizer ループは既定の学習ポリシーから始まります。これにより、適度なパフォーマンスが得られます。 [評価](concepts-offline-evaluation.md)を実行すると、Personalizer では、ループのユース ケースに合わせて最適化された新しい学習ポリシーを作成できます。 評価時に生成された特定のループごとに最適化されたポリシーを使用すると、Personalizer ははるかに優れたパフォーマンスを発揮します。

## <a name="example-use-cases-for-personalizer"></a>Personalizer のユース ケース例

* 意図の明確化と曖昧性除去: ユーザーの意図が明確でない場合に、それぞれのユーザーに合わせてパーソナル化されたオプションを提供することで、ユーザーのエクスペリエンスを向上させます。
* 既定のメニューやオプションの提案: パーソナル化されていないメニューや代替手段の一覧を表示するのではなく、ボットがパーソナル化された方法を使用して、最初の手順として最も可能性が高い項目を提案します。
* ボットの特徴とトーン: トーンや冗長性、文のスタイルといった特徴をパーソナル化された方法で変更できるボットを考えてみてください。
* 通知やアラートの内容: ユーザーのエンゲージメントを高めるために、アラートにどのようなテキストを使用するかを決定します。
* 通知とアラートのタイミング: ユーザーのエンゲージメントを高めるために、通知を送信するタイミングを個人ごとに学習させます。

## <a name="how-to-use-personalizer-in-a-web-application"></a>Web アプリケーションで Personalizer を使用する方法

Web アプリケーションにループを追加するには、以下の作業が必要です。

* どのエクスペリエンスをパーソナル化するか、どのようなアクションおよび特徴があるか、どのようなコンテキストの特徴を使用か、何を報酬に設定するかを決定します。
* アプリケーションに Personalization SDK への参照を追加します。
* パーソナル化の準備ができたら、Rank API を呼び出します。
* eventId を格納します。 後ほど、Reward API で報酬を送信します。
1. ユーザーがパーソナル化されたページを見たことを確認したら、イベントの Activate を呼び出します。
1. ユーザーがランク付けされたコンテンツを選択するのを待ちます。 
1. Rank API の出力がどの程度優れていたかを指定して Reward API を呼び出します。

## <a name="how-to-use-personalizer-with-a-chat-bot"></a>チャット ボットで Personalizer を使用する方法

この例では、一連のメニューや選択肢を毎回ユーザーに送信する代わりに、パーソナル化を使用して既定の提案を送信する方法について説明します。

* このサンプルの[コード](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/tree/master/samples/ChatbotExample)を取得します。
* ボット ソリューションをセットアップします。 LUIS アプリケーションをパブリッシュしてください。 
* ボットの Rank API および Reward API 呼び出しを使用します。
    * LUIS の意図処理を管理するコードを追加します。 最上位の意図として **None** が返された場合や、最上位の意図のスコアがビジネス ロジックのしきい値を下回った場合は、意図の一覧を Personalizer に送信して意図をランク付けします。
    * ユーザーに意図の一覧を選択可能なリンクとして表示します。その際に、最初の意図は Rank API の応答で最上位の意図とします。
    * ユーザーの選択をキャプチャし、それを Reward API 呼び出しで送信します。 

### <a name="recommended-bot-patterns"></a>推奨されるボット パターン

* 曖昧性除去が必要な場合は、ユーザーごとに結果をキャッシュするのではなく、毎回 Personalizer の Rank API を呼び出します。 あるユーザーに対する意図の曖昧性除去の結果は、時間の経過と共に変化することがあります。Rank API が差異を確認できるようにすることで、全般的な学習が高速化します。
* 十分なデータをパーソナル化に使用できるように、多くのユーザーに共通する相互作用を選択します。 たとえば、わずかなユーザーしかたどりつかない会話グラフの深い場所にある小さな明確化よりも、入門的な質問の方がうまく当てはまるかもしれません。
* ユーザーが「X はいかがですか?」と問われた場合、「最初の提案が正しい」会話になるように Rank API 呼び出しを使用します。 「X という意味ですか?」と問われた場合も同様です。 そうすると、選択肢を提示してメニューから選択してもらうのではなく、ユーザーはただ確認するだけで済みます。 たとえば、ユーザー:「コーヒーを注文したい」、ボット:「ダブル エスプレッソはいかがですか?」という形式です。 こうすることで、直接的に 1 つの提案に関連付けられるので、報酬信号も強力になります。

## <a name="how-to-use-personalizer-with-a-recommendation-solution"></a>レコメンデーション ソリューションで Personalizer を使用する方法

レコメンデーション エンジンを使用して大規模なカタログをいくつかの項目に絞り込みます。絞り込んだ項目は、Rank API に送信する 30 の候補アクションとして提示することができます。

Personalizer は、レコメンデーション エンジンと併用できます。

* [レコメンデーション ソリューション](https://github.com/Microsoft/Recommenders/)をセットアップします。 
* ページを表示するときに、レコメンデーション モデルを呼び出していくつかのおすすめ候補を取得します。
* パーソナル化を呼び出し、レコメンデーション ソリューションの出力をランク付けします。
* Reward API を呼び出し、ユーザーのアクションに関するフィードバックを送信します。


## <a name="pitfalls-to-avoid"></a>回避すべき問題

* パーソナル化された動作がすべてのユーザーで検出可能なものではなく、特定のユーザー用に記憶する必要があるような場合や、ユーザー固有の代替リストによるような場合は、Personalizer を使用しないでください。 たとえば、Personalizer を使用して 20 項目のメニューの一覧から最初に注文するピザを提案するのは便利ですが、子守を手伝ってもらいたい場合にユーザーの連絡先リストから電話をかける相手 (「おばあちゃん」など) を選ぶのは、ユーザー ベース全体でパーソナル化できるものではありません。


## <a name="adding-content-safeguards-to-your-application"></a>アプリケーションへのコンテンツ保護の追加

ユーザーに提示するコンテンツに大きな差異があるアプリケーションがあり、一部のユーザーにとって安全でないまたは不適切なコンテンツが含まれる可能性がある場合は、ユーザーが許容できないコンテンツを目にすることがないように、あらかじめ適切な保護手段を計画する必要があります。 保護手段を実装する最適なパターンは次のとおりです。
    * ランク付けするアクションの一覧を取得します。
    * 対象ユーザーが使用できないものを除外します。
    * 使用できるアクションのみをランク付けします。
    * 最上位ランクのアクションをユーザーに表示します。

アーキテクチャによっては、上記の手順を実装するのが難しい場合もあります。 その場合は、ランク付けを行った後に別の方法で保護手段を実装することができます。ただし、そのためには、保護手段で対応できないアクションが Personalizer モデルのトレーニングに使われないようにする必要があります。

* 学習を非アクティブ化した状態で、ランク付けするアクションの一覧を取得します。
* アクションをランク付けします。
* 最上位のアクションが使用できるかどうかを確認します。
    * 最上位のアクションが使用できる場合は、このランク付けの学習をアクティブ化し、ユーザーに表示します。
    * 最上位のアクションが使用できない場合は、このランク付けの学習をアクティブ化せず、独自のロジックまたは別のアプローチによってユーザーに表示するものを決定します。 次にランク付けされたオプションを使用する場合でも、このランク付けの学習をアクティブ化しないでください。

## <a name="verifying-adequate-effectiveness-of-personalizer"></a>Personalizer の有効性の検証

[オフライン評価](how-to-offline-evaluation.md)を実行することにより、Personalizer の有効性を定期的に監視できます。

## <a name="next-steps"></a>次の手順

[Personalizer を使用できる場所](where-can-you-use-personalizer.md)について理解します。
[オフライン評価](how-to-offline-evaluation.md)を実行します
