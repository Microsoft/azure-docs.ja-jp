---
title: Evaluate Model (モデルの評価):モジュール リファレンス
titleSuffix: Azure Machine Learning
description: Azure Machine Learning で Evaluate Model (モデルの評価) モジュールを使用して、トレーニング済みモデルの正確性を測定する方法について学習します。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 02/11/2020
ms.openlocfilehash: 5951c6ec63478b4b266f22eaf8bf3162e0a45df0
ms.sourcegitcommit: b95983c3735233d2163ef2a81d19a67376bfaf15
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 02/11/2020
ms.locfileid: "77137543"
---
# <a name="evaluate-model-module"></a>Evaluate Model (モデルの評価) モジュール

この記事では Azure Machine Learning デザイナー (プレビュー) 内のモジュールについて説明します。

このモジュールを使用して、トレーニング済みモデルの正確性を測定します。 モデルから生成されたスコアを含むデータセットを指定すると、**Evaluate Model (モデルの評価)** モジュールが業界標準の一連の評価メトリックを計算します。
  
 **Evaluate Model (モデルの評価)** で返されるメトリックは、評価するモデルの種類によって異なります。  
  
-   **分類モデル**    
-   **回帰モデル**    


> [!TIP]
> モデルの評価に慣れていない場合は、EdX の[機械学習コース](https://blogs.technet.microsoft.com/machinelearning/2015/09/08/new-edx-course-data-science-machine-learning-essentials/)の一部である、Stephen Elston 博士によるビデオ シリーズをお勧めします。 


**Evaluate Model (モデルの評価)** モジュールを使用するには、3 つの方法があります。

+ トレーニング データのスコアを生成し、これらのスコアに基づいてモデルを評価する
+ モデルでスコアを生成するが、これらのスコアを予約済みのテスト セットでのスコアと比較する
+ 同じデータ セットを使用して、2 つの異なるが関連するモデルのスコアを比較する

## <a name="use-the-training-data"></a>トレーニング データを使用する

モデルを評価するには、入力列とスコアのセットを含むデータセットを接続する必要があります。  他に使用できるデータがない場合は、ご自分の元のデータセットを使用できます。

1. [Score Model (モデルのスコア付け)](./score-model.md) の **Scored dataset (スコア付けされたデータセット)** 出力を **Evaluate Model (モデルの評価)** の入力に接続します。 
2. **Evaluate Model (モデルの評価)** モジュールをクリックして、パイプラインを実行して評価スコアを生成します。

## <a name="use-testing-data"></a>テスト データを使用する

機械学習における一般的なシナリオは、[Split (分割)](./split-data.md) モジュール、または [Partition and Sample (パーティションとサンプル)](./partition-and-sample.md) モジュールを使用して、元のデータ セットをトレーニングとテストのデータセットに分離することです。 

1. [Score Model (モデルのスコア付け)](score-model.md) の **Scored dataset (スコア付けされたデータセット)** 出力を **Evaluate Model (モデルの評価)** の入力に接続します。 
2. テスト データを含むデータの分割モジュールの出力を、**Evaluate Model (モデルの評価)** の右側の入力に接続します。
2. **Evaluate Model (モデルの評価)** モジュールをクリックして、 **[Run selected]\(選択項目の実行\)** を選択して評価スコアを生成します。

## <a name="compare-scores-from-two-models"></a>2 つのモデルのスコアを比較する

スコアの 2 番目のセットを **Evaluate Model (モデルの評価)** に接続することもできます。  スコアは、既知の結果または同じデータの異なるモデルからの一連の結果を使用する共有評価セットである場合があります。

この機能は、同じデータで 2 つの異なるモデルの結果を簡単に比較できるので便利です。 または、異なるパラメーターを使って同じデータに対する 2 つの異なる実行からのスコアを比較することもできます。

1. [Score Model (モデルのスコア付け)](score-model.md) の **Scored dataset (スコア付けされたデータセット)** 出力を **Evaluate Model (モデルの評価)** の入力に接続します。 
2. 2 つ目のモデルのモデルのスコア付けモジュールの出力を **Evaluate Model (モデルの評価)** の右側の入力に接続します。
3. パイプラインを実行します。

## <a name="results"></a>[結果]

**Evaluate Model (モデルの評価)** の実行後、モジュールを右クリックして **[Visualize Evaluation results]\(評価結果の視覚化\)** を選択し、結果を表示します。

データセットを **Evaluate Model (モデルの評価)** の両方の入力に接続すると、結果には両方のデータのセットまたは両方のモデルのメトリックが含まれます。
左側のポートに接続されているモデルまたはデータがレポートの先頭に表示され、その後にデータセットのメトリック、または右側のポートに接続されているモデルが表示されます。  

たとえば、次の図は、同じデータで異なるパラメーターを使用して構築された 2 つのクラスタリング モデルからの結果の比較を表しています。  

![AML&#95;Comparing2Models](media/module/aml-comparing2models.png "AML_Comparing2Models")  

これはクラスタリング モデルであるため、評価結果は 2 つの回帰モデルからのスコアを比較した場合、または 2 つの分類モデルを比較した場合とは異なります。 ただし、全体的な表示は同じです。 

## <a name="metrics"></a>メトリック

このセクションでは、**Evaluate Model (モデルの評価)** で使用するためにサポートされているモデルの特定の種類に対して返されるメトリックについて説明します。

+ [分類モデル](#metrics-for-classification-models)
+ [回帰モデル](#metrics-for-regression-models)

### <a name="metrics-for-classification-models"></a>分類モデルのメトリック

分類モデルを評価するときに、次のメトリックが報告されます。 モデルを比較すると、評価のために選択したメトリックによってモデルが順位付けされます。  
  
-   **Accuracy (正確性)** は、分類モデルの利点をケース全体に対する真の結果の割合として測定します。  
  
-   **Precision (精度)** は、すべての肯定的な結果に対する真の結果の割合です。  
  
-   **Recall (再現率)** は、モデルによって返されるすべての正しい結果の割合です。  
  
-   **F-score (F スコア)** は、精度と再現率の加重平均として、0 から 1 の範囲で計算されます。理想的な F スコアの値は 1 です。  
  
-   **AUC** は、真陽性 を x 軸に、偽陽性を y 軸にプロットした曲線の下の領域を測定します。 このメトリックは、さまざまな種類のモデルを比較できる単一の数値を提供するのに役立ちます。  
  
- **Average log loss (平均ログ損失)** は、誤った結果のペナルティを表すために使用される 1 つのスコアです。 これは 2 つの確率分布 (真の確率とモデル内の確率) の間の差として計算されます。  
  
- **Training log loss (トレーニング ログ損失)** は、ランダムな予測に対する分類子の利点を表す 1 つのスコアです。 ログ損失は、モデルがラベル内の既知の値 (グラウンド トゥルース) に出力する確率を比較することによって、モデルの不確実性を測定します。 モデル全体のログ損失を最小限にすることが目標です。

### <a name="metrics-for-regression-models"></a>回帰モデルのメトリック
 
回帰モデルに対して返されるメトリックは、エラーの量を見積もるように設計されています。  観察された値と予測された値の差が小さい場合は、モデルがデータとうまく適合しているとみなされます。 ただし、残差のパターン (任意の 1 つの予測ポイントとそれに対応する実際の値の差) を調べることで、モデル内の潜在的なバイアスに関して多くのことがわかります。  
  
 回帰モデルを評価するために次のメトリックが報告されます。 モデルを比較するときに、評価のために選択したメトリックによってモデルが順位付けされます。  
  
- **Mean absolute error (MAE) (平均絶対誤差 (MAE))** は、予測が実際の結果とどのくらい近いかを測定します。そのため、スコアは低いほど良好です。  
  
- **Root mean squared error (RMSE) (二乗平均平方根誤差 (RMSE))** は、モデル内のエラーをまとめた 1 つの値を作成します。 差を 2 乗することで、メトリックは過剰予測と過小予測との差を無視します。  
  
- **Relative absolute error (RAE) (相対絶対誤差 (RAE))** は、予測した値と実際の値との相対的な絶対差です。相対なのは、平均の差が算術平均で除算されるからです。  
  
- **Relative squared error (RSE) (相対二乗誤差 (RSE))** は、予測された値の二乗誤差の合計を、実際の値の二乗誤差の合計で除算することで、同様に正規化します。  
  

  
- **Coefficient of determination (決定係数)** (大抵は R<sup>2</sup> と呼ばれます) は、モデルの予測能力を 0 から 1 の値で表します。 ゼロはモデルがランダム (何も説明しない) であることを意味し、1 は完全一致があることを意味します。 ただし、R<sup>2</sup> 値の解釈には注意が必要です。低い値はまったく正常で、高い値は疑わしい場合があります。
  

## <a name="next-steps"></a>次のステップ

Azure Machine Learning で[使用できる一連のモジュール](module-reference.md)を参照してください。 