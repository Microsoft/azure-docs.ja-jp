---
title: WhiteNoise パッケージを使用して差分プライバシーを実装する (プレビュー)
titleSuffix: Azure Machine Learning
description: 差分プライバシーとは何かについて説明すると共に、データのプライバシーを維持する差分プライバシー システムを実装するうえで、WhiteNoise パッケージがいかに役立つかについて説明します。
author: luisquintanilla
ms.author: luquinta
ms.date: 07/09/2020
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.openlocfilehash: 9728bf2c86c0629b09e2325650ce288cf9b3cc7e
ms.sourcegitcommit: 3541c9cae8a12bdf457f1383e3557eb85a9b3187
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/09/2020
ms.locfileid: "86199804"
---
# <a name="preserve-data-privacy-by-using-differential-privacy-and-the-whitenoise-package-preview"></a>差分プライバシーと WhiteNoise パッケージを使用してデータのプライバシーを維持する (プレビュー)

差分プライバシーとは何かについて説明すると共に、差分プライバシー システムを実装するうえで WhiteNoise パッケージがいかに役立つかについて説明します。

組織では、分析用に収集/使用するデータの量が増えるにつれて、プライバシーとセキュリティについての懸念も増していきます。 分析にはデータが必要です。 通常、モデルのトレーニングに使用されるデータが多いほど、モデルの精度は高くなります。 これらの分析に個人情報が使用される場合には、データのプライバシーを維持したまま、それらのデータが使用されるようにすることが特に重要となります。

> [!NOTE]
> ツールキットの名前は変更予定で、今後数週間で新しい名前が導入されることにご注意ください。 

## <a name="how-differential-privacy-works"></a>差分プライバシーのしくみ

差分プライバシーとは、個人のデータを安全かつプライベートに保つうえで役立つ、一連のシステムとプラクティスのことです。

> [!div class="mx-imgBorder"]
> ![差分プライバシー プロセス](./media/concept-differential-privacy/differential-privacy-process.jpg)

従来のシナリオでは、生データはファイルとデータベースに保存されます。 データを分析する場合、ユーザーは通常、生データを使用します。 しかし、この方法では個人のプライバシーを侵害する可能性があるため、問題があります。 差分プライバシーは、データに "ノイズ" やランダム性を追加し、ユーザーが個々のデータ ポイントを識別できないようにすることで、この問題に対処しようとするものです。 このようなシステムにより、少なくとも一定の防御性を提供することができます。

差分プライバシー システムでは、データは**クエリ**と呼ばれる要求を通じて共有されます。 ユーザーがデータのクエリを送信すると、**プライバシー メカニズム**と呼ばれる操作によって、要求されたデータにノイズが追加されます。 プライバシー メカニズムでは、生データではなく、*データの近似値*が返されます。 このようにプライバシーを維持した結果が、**レポート**に表示されます。 レポートは、2 つの部分で構成されます。実際の計算済みデータと、データがどのように作成されたかの説明です。

## <a name="differential-privacy-metrics"></a>差分プライバシーのメトリック

差分プライバシーでは、ユーザーによってレポートが無制限に生成され、最終的に機密データが漏洩するのを防ぐための措置が講じられます。 レポートのノイズがどの程度で、プライバシーがどの程度確保されるかという度合いは、**epsilon** という値によって測定されます。 epsilon は、ノイズやプライバシーの度合いに反比例します。 epsilon が低いほど、データはノイズが高く (プライベートに) なります。

epsilon の値は、負以外の値となります。 1 未満の値を指定すると、完全な防御性が得られます。 1 より大きい値にすると、実際のデータが危険にさらされる可能性がより高くなります。 差分プライバシー システムを実装する際には、epsilon の値を 0 - 1 の範囲に設定してレポートを作成する必要があります。

epsilon に直接関連付けられているもう 1 つの値が、**delta** です。 delta は、レポートが完全にプライベートでない可能性を示す尺度です。 delta が高いほど、epsilon は大きくなります。 これらの値には相関関係があるため、epsilon のほうがより頻繁に使用されます。

## <a name="privacy-budget"></a>プライバシー予算

差分プライバシーでは、複数のクエリが許可されているシステムでプライバシーを維持するために、レート制限が定義されます。 この制限は、**プライバシー予算**と呼ばれます。 プライバシー予算には epsilon (通常は 1 - 3 の範囲) が割り当てられ、これによって再識別のリスクが制限されます。 レポートが生成される際には、プライバシー予算によって、個々のレポートの epsilon 値と、すべてのレポートの集計が追跡されます。 プライバシー予算を使い果たすと、ユーザーはデータにアクセスできなくなります。  

## <a name="reliability-of-data"></a>データの信頼性

プライバシーの維持を目標とすることは必要ですが、データのユーザビリティと信頼性に関しては、あるトレードオフが存在します。 データ分析の場合、精度とは、サンプリング エラーによって生じる不確実性の尺度と考えることができます。 この不確実性は、特定の範囲内に収まる傾向があります。 一方、差分プライバシーの観点から見た場合、**精度**とは、データの信頼性を測る尺度となります。そしてこれは、プライバシー メカニズムによって生じる不確実性の影響を受けます。 つまり、ノイズやプライバシーのレベルが高いほど、データの epsilon、精度、および信頼性は低くなるということです。 データのプライバシーは増しますが、信頼性が低下するため、使用される可能性は低くなります。

## <a name="implementing-differentially-private-systems"></a>差分プライバシー システムの実装

差分プライバシー システムを実装することは簡単ではありません。 WhiteNoise は、グローバルな差分プライバシー システムを構築するためのさまざまなコンポーネントを含んだ、オープンソース プロジェクトです。 WhiteNoise を構成している最上位レベルのコンポーネントは次のとおりです。

- コア
- システム

### <a name="core"></a>コア

コア ライブラリには、差分プライバシー システムを実装するための、次のプライバシー メカニズムが含まれています。

|コンポーネント  |説明  |
|---------|---------|
|分析     | 任意の計算のグラフ記述。 |
|検証コントロール     | 分析を差分プライベートにするために必要な条件を確認して抽出するための、一連のツールを含んだ Rust ライブラリ。          |
|ランタイム     | 分析を実行するメディア。 参照ランタイムは Rust で記述されますが、ランタイムは、データのニーズに応じて任意の計算フレームワークを使用して記述できます (SQL や Spark など)。        |
|バインド     | 分析を構築するための言語バインドとヘルパー ライブラリ。 現在、WhiteNoise では Python のバインドが提供されています。 |

### <a name="system"></a>システム

システム ライブラリでは、表形式データとリレーショナル データを操作するための、次のツールとサービスが提供されています。

|コンポーネント  |説明  |
|---------|---------|
|[データ アクセス]     | SQL クエリをインターセプトして処理し、レポートを生成するライブラリ。 このライブラリは Python で実装されます。次の ODBC および DBAPI データソースがサポートされています。<ul><li>PostgreSQL</li><li>SQL Server</li><li>Spark</li><li>Preston</li><li>Pandas</li></ul>|
|サービス     | 共有データソースに対する要求やクエリを処理する REST エンドポイントを提供する実行サービス。 このサービスは、さまざまな delta 値と epsilon 値 (異種要求とも呼ばれます) を含んだ要求に対して動作する、差分プライバシー モジュールを構成できるように設計されています。 この参照実装は、関連付けられたデータに対するクエリから受ける追加的な影響に対応します。 |
|エバリュエーター     | プライバシー違反、精度、およびバイアスをチェックするストキャスティクス エバリュエーター。 エバリュエーターでは、次のテストがサポートされています。 <ul><li>プライバシー テスト - レポートが差分プライバシーの条件に準拠しているかどうかを判断します。</li><li>正確性テスト - レポートの信頼性が、95% の信頼度レベルで上限と下限の範囲内であるかどうかを測定します。</li><li>ユーティリティ テスト - レポートの信頼度の範囲がデータに対して十分に近いかどうかを判断しながら、プライバシーを最大限に高めます。</li><li>バイアス テスト - レポートの分布を測定してクエリの繰り返しをチェックし、不均衡を防ぎます</li></ul> |

## <a name="next-steps"></a>次のステップ

Azure Machine Learning で[データのプライバシーを維持](how-to-differential-privacy.md)します。

WhiteNoise のコンポーネントの詳細については、[WhiteNoise コア パッケージ](https://github.com/opendifferentialprivacy/whitenoise-core)、[WhiteNoise システム パッケージ](https://github.com/opendifferentialprivacy/whitenoise-system)および [WhiteNoise サンプル](https://github.com/opendifferentialprivacy/whitenoise-samples)の GitHub リポジトリを参照してください。